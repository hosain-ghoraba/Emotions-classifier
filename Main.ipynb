{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "\n",
        "revise get clusters names\n",
        "\n",
        "\n",
        "revise get accuracy , and think what will happen if missing cluster names\n",
        "report both models results on some samples from test data\n",
        "<br>\n",
        "\n",
        "modify dataset perc to use\n",
        "\n",
        "Train/test timing\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Milestone 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Packages installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install kagglehub\n",
        "!pip install imagehash\n",
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "print(\"done installing packages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import random\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.decomposition import PCA\n",
        "from time import time\n",
        "\n",
        "print(\"done importing packages\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Data preparaion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "    dataset_folder = kagglehub.dataset_download('hussainghoraba/emotions-dataset')\n",
        "    DATASET_PATH = os.path.join(dataset_folder, 'Dataset')\n",
        "elif 'KAGGLE_URL_BASE' in os.environ:\n",
        "    DATASET_PATH = '/kaggle/input/emotions-dataset/Dataset'\n",
        "elif 'VSCODE_PID' in os.environ:\n",
        "    DATASET_PATH = './Dataset'\n",
        "else:\n",
        "    raise Exception('Unknown environment')\n",
        "\n",
        "print(\"done setting up dataset path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set random seed & some global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "TAREGT_SIZE_TUPLE = (512, 512)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
        "print(\"done setting up random seed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_and_print_time(start_time, section_name):\n",
        "    end_time = time()\n",
        "    time_taken = end_time - start_time\n",
        "    hours = int(time_taken // 3600)\n",
        "    minutes = int((time_taken % 3600) // 60)\n",
        "    seconds = int((time_taken % 3600) % 60)\n",
        "    print(f'\\n{section_name} Done in : {hours} h, {minutes} m, {seconds} s')\n",
        "    \n",
        "def extend_depth(filters, depth):\n",
        "    return np.stack([filters] * depth, axis=-1)\n",
        "\n",
        "def get_image_flattened_windows(input_data, filter_size):\n",
        "    input_h, input_w, input_c = input_data.shape\n",
        "    output_h = input_h - filter_size + 1\n",
        "    output_w = input_w - filter_size + 1\n",
        "\n",
        "    col = np.zeros((output_h * output_w, filter_size * filter_size * input_c))\n",
        "    i = 0\n",
        "    for h in range(output_h):\n",
        "        for w in range(output_w):\n",
        "            cube = input_data[h:h+filter_size, w:w+filter_size, :]\n",
        "            col[i, :] = cube.flatten()\n",
        "            i += 1\n",
        "    return col\n",
        "\n",
        "def downsample_features(features_arrays):\n",
        "    # downsample the features arrays to only 128 dimensions\n",
        "    # use PCA only if we have more than 128 feature arrays ,\n",
        "    # this is because in PCA, the number of components (which is 128 in our case) \n",
        "    # must be less than the number of samples.\n",
        "    # if we have less than 128 feature arrays, we will just select random 128 features\n",
        "    if features_arrays.shape[0] >= 128:\n",
        "        print(f\"appling PCA...\")\n",
        "        features_arrays = PCA(n_components=128).fit_transform(features_arrays)\n",
        "    else:\n",
        "        print(f\"selecting random 128 features from {features_arrays.shape[0]} features\")\n",
        "        selected_indices = np.random.choice(features_arrays.shape[1], 128, replace=False)\n",
        "        features_arrays = features_arrays[:, selected_indices]\n",
        "    return features_arrays\n",
        "\n",
        "def extract_features(model, df):\n",
        "    start_time = time()\n",
        "    num_features = model.layers[-1].output_shape[0]\n",
        "    features_arrays = np.zeros((len(df), num_features))\n",
        "\n",
        "    with tqdm(total=len(df), desc=\"Extracting features...\") as pbar:\n",
        "        for i in range(len(df)):\n",
        "            img_arr = df.iloc[i]['img_arr']\n",
        "            features_arrays[i, :] = model.forward(img_arr)\n",
        "            pbar.update(1)\n",
        "    calculate_and_print_time(start_time, \"Feature Extraction without downsampling\")\n",
        "    return downsample_features(features_arrays)\n",
        "\n",
        "def calculate_accuracy(true_labels, predicted_labels):\n",
        "    true_labels = np.array(true_labels)\n",
        "    predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "    correct_predictions = np.sum(true_labels == predicted_labels)\n",
        "    accuracy = correct_predictions / len(true_labels)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset into memory without dups, and with correct size, and equalize the number of images in each class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "global_start_time = time()\n",
        "total_images_count = sum(len(files) for _, _, files in os.walk(DATASET_PATH))\n",
        "print(f\"Total images in dataset: {total_images_count}\")\n",
        "train_features_arrays = []\n",
        "dups_pairs = set()\n",
        "\n",
        "# load only a small percentage of the dataset, for faster testing while developing\n",
        "DATASET_PERC_TO_USE = 1\n",
        "\n",
        "num_images_in_smallest_category = min(len(os.listdir(os.path.join(DATASET_PATH, folder))) for folder in os.listdir(DATASET_PATH))\n",
        "num_of_images_to_use_in_each_category = int(num_images_in_smallest_category * DATASET_PERC_TO_USE)\n",
        "num_of_categories = len(os.listdir(DATASET_PATH))\n",
        "total_images_to_load = int(num_images_in_smallest_category * num_of_categories * DATASET_PERC_TO_USE)\n",
        "\n",
        "with tqdm(total=total_images_to_load, desc=\"Loading images into memory...\") as pbar:\n",
        "    for subfolder in os.listdir(DATASET_PATH):\n",
        "        subfolder_path = os.path.join(DATASET_PATH, subfolder)\n",
        "        subfolder_hashes = {}\n",
        "\n",
        "        all_category_images = os.listdir(subfolder_path)\n",
        "        # we must use the same number of images from each category to avoid bias\n",
        "        images_to_load = random.sample(all_category_images, num_of_images_to_use_in_each_category)\n",
        "        \n",
        "        for img_file in images_to_load:\n",
        "            img_path = os.path.join(subfolder_path, img_file)\n",
        "            with Image.open(img_path) as img:\n",
        "                img = img.convert(\"RGB\").resize(TAREGT_SIZE_TUPLE)\n",
        "                img_arr = np.array(img)\n",
        "                img_hash = imagehash.phash(img)\n",
        "            if img_hash not in subfolder_hashes.keys():\n",
        "                train_features_arrays.append({\"img_path\": img_path, \"label\": subfolder, \"img_arr\": img_arr})\n",
        "                # key : hash, value : img_path\n",
        "                subfolder_hashes[img_hash] = img_path\n",
        "            else:\n",
        "                existing_duplicate = subfolder_hashes[img_hash]\n",
        "                dups_pairs.add((img_path, existing_duplicate))\n",
        "            pbar.update(1)\n",
        "        \n",
        "df = pd.DataFrame(train_features_arrays)\n",
        "\n",
        "# display dups\n",
        "for dup_pair in dups_pairs:\n",
        "    print(f\"Duplicate images found: {dup_pair[0]} and {dup_pair[1]}\")\n",
        "    img1 = Image.open(dup_pair[0])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img1)\n",
        "    plt.title(os.path.basename(dup_pair[0]))\n",
        "    plt.axis('off')\n",
        "    img2 = Image.open(dup_pair[1])\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img2)\n",
        "    plt.title(os.path.basename(dup_pair[1]))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "calculate_and_print_time(global_start_time, \"Loading images into memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Test/Val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C_b_7bkU03Y",
        "outputId": "7f2ab7fd-d76a-42c0-ac0d-fee917e716d9"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=(1 - train_ratio), stratify=df['label'], random_state=RANDOM_SEED)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=(test_ratio / (test_ratio + val_ratio)), stratify=temp_df['label'], random_state=RANDOM_SEED)\n",
        "\n",
        "# Print the sizes of each split\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "print(\"\\ntraining set:\")\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "print(\"\\nvalidation set:\")\n",
        "print(val_df['label'].value_counts())\n",
        "\n",
        "print(\"\\ntest set:\")\n",
        "print(test_df['label'].value_counts())\n",
        "\n",
        "\n",
        "print(\"done splitting dataset into train, val, test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgDAdbjXbtMy"
      },
      "source": [
        "## **Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classes & Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvLayer:\n",
        "    def __init__(self, filter_size=3, num_filters=5, filter_weights=None):\n",
        "        self.filter_size = filter_size\n",
        "        self.num_filters = num_filters\n",
        "        if filter_weights is None:\n",
        "            filter_weights = np.random.normal(size=(num_filters, filter_size, filter_size)) * 0.1\n",
        "        self.filter_weights = filter_weights\n",
        "\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "        self.deep_filters = None\n",
        "        self.deep_filters_shape = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        input_h, input_w, input_c = input_shape\n",
        "\n",
        "        # ------------------- define the output shape \n",
        "        output_h = input_h - self.filter_size + 1\n",
        "        output_w = input_w - self.filter_size + 1\n",
        "        self.output_shape = (output_h, output_w, self.num_filters)\n",
        "\n",
        "        # ------------------- initilize deep filters \n",
        "        #  extend the filter weights depth so that filter depth = input depth (input channels)\n",
        "        self.deep_filters = extend_depth(self.filter_weights, input_c)\n",
        "        self.deep_filters_shape = self.deep_filters.shape\n",
        "        # flatten the filters for fast matrix multiplication in the \"forward\" fuction\n",
        "        self.deep_filters = self.deep_filters.reshape(self.num_filters, -1)\n",
        "\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        output_h, output_w = self.output_shape[0], self.output_shape[1]\n",
        "        input_col = get_image_flattened_windows(input_image, self.filter_size)\n",
        "        output_flat = self.deep_filters @ input_col.T\n",
        "        output = output_flat.T.reshape(output_h, output_w, self.num_filters)\n",
        "        return output\n",
        "    \n",
        "\n",
        "class PoolingLayer:\n",
        "    def __init__(self, pool_size=2, pool_type='MAX'):\n",
        "        if pool_type not in ['MAX', 'AVERAGE']:\n",
        "            raise ValueError(\"pool_type must be either 'MAX' or 'AVERAGE'\")\n",
        "        self.pool_size = pool_size\n",
        "        self.pool_type = pool_type\n",
        "\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "        self.output = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        input_h, input_w, input_c = input_shape\n",
        "\n",
        "        # define the output shape\n",
        "        output_h = input_h // self.pool_size\n",
        "        output_w = input_w // self.pool_size\n",
        "        self.output_shape = (output_h, output_w, input_c)\n",
        "\n",
        "        # initialize the output array\n",
        "        self.output = np.zeros(self.output_shape)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        for i in range(self.output.shape[0]):\n",
        "            for j in range(self.output.shape[1]):\n",
        "                h_start = i * self.pool_size\n",
        "                h_end = h_start + self.pool_size\n",
        "                w_start = j * self.pool_size\n",
        "                w_end = w_start + self.pool_size\n",
        "\n",
        "                pool_window = input_image[h_start:h_end, w_start:w_end, :]\n",
        "\n",
        "                if self.pool_type == 'MAX':\n",
        "                    self.output[i, j, :] = np.max(pool_window, axis=(0, 1))\n",
        "                elif self.pool_type == 'AVERAGE':\n",
        "                    self.output[i, j, :] = np.mean(pool_window, axis=(0, 1))\n",
        "\n",
        "        # apply relu\n",
        "        return np.maximum(0, self.output)\n",
        "    \n",
        "class FlatteningLayer:\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = (input_shape[0] * input_shape[1] * input_shape[2],)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        return input_image.flatten()\n",
        "\n",
        "\n",
        "class Model_1:\n",
        "    def __init__(self, layers, first_layer_input_shape):\n",
        "        self.layers = layers\n",
        "\n",
        "        # Set the input shape for each layer based on the output shape of the previous layer\n",
        "        previous_layer_shape = first_layer_input_shape\n",
        "        for layer in self.layers:\n",
        "            layer.set_input_shape(previous_layer_shape)\n",
        "            previous_layer_shape = layer.output_shape  \n",
        "\n",
        "    def visualize_architecture(self):\n",
        "        print(\"Model architecture : üõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏èüõ†Ô∏è\\n\")\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(f\"Layer {i + 1}: {layer.__class__.__name__}\")\n",
        "            print(f\"Input shape: {layer.input_shape}\")\n",
        "            print(f\"Output shape: {layer.output_shape}\")\n",
        "            if isinstance(layer, ConvLayer):\n",
        "                print(f\"Number of filters: {layer.deep_filters_shape[0]}\")\n",
        "                print(f\"Filter Shape: {layer.deep_filters_shape[-3:]}\")\n",
        "            elif isinstance(layer, PoolingLayer):\n",
        "                print(f\"Pooling size: {layer.pool_size}\")\n",
        "                print(f\"Pooling type: {layer.pool_type}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        output = input_image\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "class Custom_Kmeans:\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "        self.centroids = None\n",
        "        self.clusters_names = None\n",
        "        self.index_labels = None\n",
        "\n",
        "    def predict(self, feature_arr):\n",
        "        distances = np.linalg.norm(feature_arr - self.centroids, axis=1)\n",
        "        cluster_index = np.argmin(distances)\n",
        "\n",
        "        # replace every index label with the name of the centroid, put in a new array\n",
        "        # exmaple : convert [0, 1, 2, 3] to ['happy', 'sad', 'angry', 'neutral']\n",
        "        return self.clusters_names[cluster_index]\n",
        "    \n",
        "    def get_clusters_names(self, true_labels):\n",
        "        num_of_classes = len(np.unique(true_labels))\n",
        "        cluster_names = []\n",
        "        for i in range(num_of_classes):\n",
        "            cluster_labels = true_labels[self.index_labels == i]\n",
        "            # print number of images of each class in each cluster\n",
        "            print(f\"Cluster {i}:\")\n",
        "            print(pd.Series(cluster_labels).value_counts())\n",
        "            print(\"-\" * 30)\n",
        "            most_common_label = pd.Series(cluster_labels).mode()[0]\n",
        "            cluster_names.append(most_common_label)\n",
        "        return np.array(cluster_names)\n",
        "    \n",
        "    def fit(self, train_features_arrays, train_features_arrays_labels, val_features_arrays, val_features_arrays_labels):\n",
        "        # features_arrays is a 2D array of shape (num_images, num_features)\n",
        "        # features_arrays_labels is a 1D array of shape (num_samples,), \n",
        "        # representing the labels of images whose features are passed as features_arrays\n",
        "        rows_count = train_features_arrays.shape[0]\n",
        "\n",
        "        # Initialize centroids randomly    gg\n",
        "        self.centroids = train_features_arrays[np.random.choice(rows_count, self.k, replace=False)]\n",
        "\n",
        "        iterations_accuracies = []\n",
        "        while True:\n",
        "            distances = np.linalg.norm(train_features_arrays[:, np.newaxis] - self.centroids, axis=2)\n",
        "            self.index_labels = np.argmin(distances, axis=1) \n",
        "            previous_centroids = self.centroids.copy()\n",
        "            self.centroids = np.array([train_features_arrays[self.index_labels == i].mean(axis=0) for i in range(self.k)])\n",
        "            \n",
        "            # name each cluster as the name of the most frequent label in the cluster\n",
        "            self.clusters_names = self.get_clusters_names(train_features_arrays_labels)\n",
        "\n",
        "            # predict the labels of the validation set\n",
        "            val_predicted_labels = np.array([self.predict(val_features_arrays[i]) for i in range(len(val_features_arrays))])\n",
        "\n",
        "            # calculate the accuracy\n",
        "            val_accuracy = calculate_accuracy(val_features_arrays_labels, val_predicted_labels)\n",
        "\n",
        "            iterations_accuracies.append(val_accuracy)\n",
        "\n",
        "            # breack if difference between new and old centroids is small\n",
        "            tolerance = 1e-4\n",
        "            if np.all(np.linalg.norm(self.centroids - previous_centroids, axis=1) < tolerance):\n",
        "                break\n",
        "        return iterations_accuracies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the model to extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filter_weights = np.stack([\n",
        "    np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]),\n",
        "    np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),\n",
        "    np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n",
        "    np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),\n",
        "    np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "], axis=0)\n",
        "\n",
        "model_1 = Model_1(\n",
        "    layers=[\n",
        "        ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights),\n",
        "        PoolingLayer(pool_size=2, pool_type='MAX'),\n",
        "        ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights),\n",
        "        PoolingLayer(pool_size=2, pool_type='MAX'),\n",
        "        FlatteningLayer()\n",
        "    ],\n",
        "    first_layer_input_shape=df.iloc[0]['img_arr'].shape\n",
        ")\n",
        "model_1.visualize_architecture()\n",
        "\n",
        "\n",
        "train_features_arrays = extract_features(model_1, train_df)\n",
        "val_features_arrays = extract_features(model_1, val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = len(df['label'].unique())\n",
        "train_features_arrays_labels = train_df['label'].values\n",
        "val_features_arrays_labels = val_df['label'].values\n",
        "\n",
        "kmeans = Custom_Kmeans(k=num_classes)\n",
        "clustering_history = kmeans.fit(train_features_arrays, train_features_arrays_labels, val_features_arrays, val_features_arrays_labels)\n",
        "print(f\"clustering done in {len(clustering_history)} iterations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7VknR2cbydf"
      },
      "source": [
        "## **Model 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "xFDVzn_qb359",
        "outputId": "a0c234e1-4fc7-4dfb-f08e-32af3e76f40a"
      },
      "outputs": [],
      "source": [
        "# TA Dinah : \"You should have 5 convolution blocks each with 3 layers.\n",
        "#  3 convolution layers are simply 3 different filters on the same stage.\n",
        "#  A convolution block is some conv filters (layers) followed by an activation function and then a max pooling. \n",
        "# All convolution filters in the same block need to have the same size.\"\n",
        "\n",
        "def create_model2():\n",
        "    model = models.Sequential([\n",
        "    \n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid', input_shape=(TAREGT_SIZE_TUPLE[0], TAREGT_SIZE_TUPLE[1], 3)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    # 128 is OUR CHOICE for the number of neurons in the hidden layer (not specified in the project description)\n",
        "    layers.Dense(128, activation='sigmoid'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model2 = create_model2()\n",
        "\n",
        "\n",
        "print(\"done building model 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Early stopping to prevent overfitting (for the BONUS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                            patience=2,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr = 0.00001,\n",
        "                                            verbose = 1)\n",
        "\n",
        "early_stoping = EarlyStopping(monitor='val_loss',patience= 3,restore_best_weights=True,verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data augmentation (for the BONUS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# thiss will rotate the images by up to 20 degrees, also will increase the \n",
        "# dataset size on the fly while training \n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4GKqc0Yfl5e",
        "outputId": "7862dc2e-3b93-4508-d04a-01be54e7f565"
      },
      "outputs": [],
      "source": [
        "# start_time = time()\n",
        "# batch_size = 16\n",
        "# epochs = 10\n",
        "\n",
        "# # print trining dataset size before and after augmentation\n",
        "# print(f\"Training dataset size before augmentation: {len(train_df)}\")\n",
        "# train_gen = train_datagen.flow_from_dataframe(\n",
        "#     dataframe=train_df,\n",
        "#     target_size=TAREGT_SIZE_TUPLE,\n",
        "#     x_col='img_path',  \n",
        "#     y_col='label',    \n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical', \n",
        "#     shuffle=True\n",
        "# )\n",
        "# print(f\"Training dataset size after augmentation: {train_gen.n}\")\n",
        "\n",
        "# val_gen = val_datagen.flow_from_dataframe(\n",
        "#     dataframe=val_df,\n",
        "#     target_size=TAREGT_SIZE_TUPLE,\n",
        "#     x_col='img_path',\n",
        "#     y_col='label',\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical',\n",
        "#     shuffle=False\n",
        "# )\n",
        "# print(f\"images sizes passed to image data generator: {train_gen.image_shape}\")\n",
        "\n",
        "# test_gen = test_datagen.flow_from_dataframe(\n",
        "#     dataframe=test_df,\n",
        "#     target_size=TAREGT_SIZE_TUPLE,\n",
        "#     x_col='img_path',\n",
        "#     y_col='label',\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical',\n",
        "#     shuffle=False\n",
        "# )\n",
        "\n",
        "\n",
        "# model2_history = model2.fit(\n",
        "#     train_gen,\n",
        "#     validation_data=val_gen,\n",
        "#     callbacks=[early_stoping, learning_rate_reduction],\n",
        "#     epochs=epochs,\n",
        "# )\n",
        "\n",
        "# test_loss, test_acc = model2.evaluate(test_gen)\n",
        "# print(f\"Val accuracy: {model2_history.history['val_accuracy'][-1]:.2f}\")\n",
        "# print(f\"Test accuracy: {test_acc:.2f}\")\n",
        "\n",
        "\n",
        "# calculate_and_print_time(start_time, \"Training model 2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "JMOPXDv7hYqe",
        "outputId": "f4a6c5ed-9f4e-4057-9374-fa663d72add1"
      },
      "outputs": [],
      "source": [
        "# # Get class indices and reverse mapping\n",
        "# class_indices = test_gen.class_indices\n",
        "# class_labels = list(class_indices.keys())\n",
        "\n",
        "# # Get predictions for the test set\n",
        "# preds = model.predict(test_gen)\n",
        "# predicted_classes = np.argmax(preds, axis=1)\n",
        "# true_classes = test_gen.classes\n",
        "# filenames = test_gen.filenames\n",
        "\n",
        "# # Display 10 random images with predictions\n",
        "# num_images = 10\n",
        "# indices = np.random.choice(len(filenames), num_images, replace=False)\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "\n",
        "# for i, idx in enumerate(indices):\n",
        "#     img_path = os.path.join(test_dir, filenames[idx])\n",
        "#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
        "#     img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "\n",
        "#     plt.subplot(2, 5, i + 1)\n",
        "#     plt.imshow(img_array)\n",
        "#     plt.axis('off')\n",
        "#     true_label = class_labels[true_classes[idx]]\n",
        "#     predicted_label = class_labels[predicted_classes[idx]]\n",
        "#     title_color = \"green\" if true_label == predicted_label else \"red\"\n",
        "#     plt.title(f\"True: {true_label}\\nPred: {predicted_label}\", color=title_color)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "print(\"done displaying predictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Milestone 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 1**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy vs iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 2**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy vs iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.plot(model2_history.history['accuracy'], label='train accuracy')\n",
        "# plt.plot(model2_history.history['val_accuracy'], label='val accuracy')\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# start_time = time()\n",
        "# k = 4\n",
        "# kf = KFold(n_splits=k, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "# fold_accuracies = []\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "#     print(f\"\\nStarting fold {fold + 1}/{k}...\")\n",
        "    \n",
        "#     train_data = df.iloc[train_idx]\n",
        "#     val_data = df.iloc[val_idx]\n",
        "    \n",
        "#     train_gen = train_datagen.flow_from_dataframe(\n",
        "#         dataframe=train_data,\n",
        "#         x_col='img_path',\n",
        "#         y_col='label',\n",
        "#         target_size=TAREGT_SIZE_TUPLE,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode='categorical',\n",
        "#         shuffle=True\n",
        "#     )\n",
        "    \n",
        "#     val_gen = val_datagen.flow_from_dataframe(\n",
        "#         dataframe=val_data,\n",
        "#         x_col='img_path',\n",
        "#         y_col='label',\n",
        "#         target_size=TAREGT_SIZE_TUPLE,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode='categorical',\n",
        "#         shuffle=False\n",
        "#     )\n",
        "    \n",
        "#     model2 = create_model2()\n",
        "    \n",
        "#     model2_history = model2.fit(\n",
        "#         train_gen,\n",
        "#         validation_data=val_gen,\n",
        "#         callbacks=[early_stoping, learning_rate_reduction],\n",
        "#         epochs=epochs,\n",
        "#     )\n",
        "    \n",
        "#     val_accuracy = model2.evaluate(val_gen)[1]\n",
        "#     print(f\"\\nFold {fold + 1} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "#     fold_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "# average_accuracy = sum(fold_accuracies) / k\n",
        "# print(f\"All folds validation accuracies: {fold_accuracies}\")\n",
        "# print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
        "\n",
        "\n",
        "# calculate_and_print_time(start_time, \"Model 2 K-fold cross-validation\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def display_confusion_matrix(model, val_gen, plt_title):\n",
        "#     y_true = val_gen.classes\n",
        "#     print(f\"y_true : {y_true}\")\n",
        "#     y_pred = model.predict(val_gen)\n",
        "#     # print(f\"y_pred : {y_pred}\")\n",
        "#     y_pred_classes = y_pred.argmax(axis=1)\n",
        "    \n",
        "#     cm = confusion_matrix(y_true, y_pred_classes)\n",
        "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_gen.class_indices.keys())\n",
        "#     disp.plot(cmap='viridis')\n",
        "#     plt.title(plt_title)\n",
        "#     plt.show()\n",
        "\n",
        "# display_confusion_matrix(model2, val_gen, \"Confusion Matrix for model 2\")\n",
        "# calculate_and_print_time(global_start_time, \"All Notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### notes : model2 problem is not in the model architecture (varified this by trying on bachelor model architecture, got similar results) so the problem is most likely in the data, or some where else in the code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
