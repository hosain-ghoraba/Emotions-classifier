{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "\n",
        "revise get clusters names\n",
        "\n",
        "revise get accuracy , and think what will happen if missing cluster names\n",
        "report both models results on some samples from test data\n",
        "<br>\n",
        "\n",
        "modify dataset perc to use\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Milestone 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Packages installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in ./.venv/lib/python3.12/site-packages (0.3.11)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (2025.1.31)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: imagehash in ./.venv/lib/python3.12/site-packages (4.3.2)\n",
            "Requirement already satisfied: PyWavelets in ./.venv/lib/python3.12/site-packages (from imagehash) (1.8.0)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from imagehash) (2.1.3)\n",
            "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (from imagehash) (11.2.1)\n",
            "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from imagehash) (1.15.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: tensorflow in ./.venv/lib/python3.12/site-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in ./.venv/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.venv/lib/python3.12/site-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from tensorflow) (78.1.1)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.67.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "done installing packages\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub\n",
        "!pip install imagehash\n",
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "print(\"done installing packages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done importing packages\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import random\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.decomposition import PCA\n",
        "from time import time\n",
        "\n",
        "print(\"done importing packages\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Data preparaion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done setting up dataset path\n"
          ]
        }
      ],
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "    dataset_folder = kagglehub.dataset_download('hussainghoraba/emotions-dataset')\n",
        "    DATASET_PATH = os.path.join(dataset_folder, 'Dataset')\n",
        "elif 'KAGGLE_URL_BASE' in os.environ:\n",
        "    DATASET_PATH = '/kaggle/input/emotions-dataset/Dataset'\n",
        "elif 'VSCODE_PID' in os.environ:\n",
        "    DATASET_PATH = './Dataset'\n",
        "else:\n",
        "    raise Exception('Unknown environment')\n",
        "\n",
        "print(\"done setting up dataset path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set random seed & some global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done setting up random seed\n"
          ]
        }
      ],
      "source": [
        "RANDOM_SEED = 42\n",
        "TAREGT_SIZE_TUPLE = (512, 512)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
        "print(\"done setting up random seed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_and_print_time(start_time, section_name):\n",
        "    end_time = time()\n",
        "    time_taken = end_time - start_time\n",
        "    hours = int(time_taken // 3600)\n",
        "    minutes = int((time_taken % 3600) // 60)\n",
        "    seconds = int((time_taken % 3600) % 60)\n",
        "    print(f'\\n{section_name} Done in : {hours} h, {minutes} m, {seconds} s')\n",
        "    \n",
        "def extend_depth(filters, depth):\n",
        "    return np.stack([filters] * depth, axis=-1)\n",
        "\n",
        "def get_image_flattened_windows(input_data, filter_size):\n",
        "    input_h, input_w, input_c = input_data.shape\n",
        "    output_h = input_h - filter_size + 1\n",
        "    output_w = input_w - filter_size + 1\n",
        "\n",
        "    col = np.zeros((output_h * output_w, filter_size * filter_size * input_c))\n",
        "    i = 0\n",
        "    for h in range(output_h):\n",
        "        for w in range(output_w):\n",
        "            cube = input_data[h:h+filter_size, w:w+filter_size, :]\n",
        "            col[i, :] = cube.flatten()\n",
        "            i += 1\n",
        "    return col\n",
        "\n",
        "def downsample_features(features_arrays):\n",
        "    # downsample the features arrays to only 128 dimensions\n",
        "    # use PCA only if we have more than 128 feature arrays ,\n",
        "    # this is because in PCA, the number of components (which is 128 in our case) \n",
        "    # must be less than the number of samples.\n",
        "    # if we have less than 128 feature arrays, we will just select random 128 features\n",
        "    if features_arrays.shape[0] >= 128:\n",
        "        print(f\"appling PCA...\")\n",
        "        features_arrays = PCA(n_components=128).fit_transform(features_arrays)\n",
        "    else:\n",
        "        print(f\"selecting random 128 features from {features_arrays.shape[0]} features\")\n",
        "        selected_indices = np.random.choice(features_arrays.shape[1], 128, replace=False)\n",
        "        features_arrays = features_arrays[:, selected_indices]\n",
        "    return features_arrays\n",
        "\n",
        "def extract_features(model, df):\n",
        "    start_time = time()\n",
        "    num_features = model.layers[-1].output_shape[0]\n",
        "    features_arrays = np.zeros((len(df), num_features))\n",
        "\n",
        "    with tqdm(total=len(df), desc=\"Extracting features...\") as pbar:\n",
        "        for i in range(len(df)):\n",
        "            img_arr = df.iloc[i]['img_arr']\n",
        "            features_arrays[i, :] = model.forward(img_arr)\n",
        "            pbar.update(1)\n",
        "    calculate_and_print_time(start_time, \"Feature Extraction without downsampling\")\n",
        "    return downsample_features(features_arrays)\n",
        "\n",
        "def calculate_accuracy(true_labels, predicted_labels):\n",
        "    true_labels = np.array(true_labels)\n",
        "    predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "    correct_predictions = np.sum(true_labels == predicted_labels)\n",
        "    accuracy = correct_predictions / len(true_labels)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset into memory without dups, and with correct size, and equalize the number of images in each class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images in dataset: 2125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading images into memory...:  98%|█████████▊| 116/118 [00:01<00:00, 82.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading images into memory Done in : 0 h, 0 m, 1 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "global_start_time = time()\n",
        "total_images_count = sum(len(files) for _, _, files in os.walk(DATASET_PATH))\n",
        "print(f\"Total images in dataset: {total_images_count}\")\n",
        "train_features_arrays = []\n",
        "dups_pairs = set()\n",
        "\n",
        "# load only a small percentage of the dataset, for faster testing while developing\n",
        "DATASET_PERC_TO_USE = 0.1\n",
        "  \n",
        "num_images_in_smallest_category = min(len(os.listdir(os.path.join(DATASET_PATH, folder))) for folder in os.listdir(DATASET_PATH))\n",
        "num_of_images_to_use_in_each_category = int(num_images_in_smallest_category * DATASET_PERC_TO_USE)\n",
        "num_of_categories = len(os.listdir(DATASET_PATH))\n",
        "total_images_to_load = int(num_images_in_smallest_category * num_of_categories * DATASET_PERC_TO_USE)\n",
        "\n",
        "with tqdm(total=total_images_to_load, desc=\"Loading images into memory...\") as pbar:\n",
        "    for subfolder in os.listdir(DATASET_PATH):\n",
        "        subfolder_path = os.path.join(DATASET_PATH, subfolder)\n",
        "        subfolder_hashes = {}\n",
        "\n",
        "        all_category_images = os.listdir(subfolder_path)\n",
        "        # we must use the same number of images from each category to avoid bias\n",
        "        images_to_load = random.sample(all_category_images, num_of_images_to_use_in_each_category)\n",
        "        \n",
        "        for img_file in images_to_load:\n",
        "            img_path = os.path.join(subfolder_path, img_file)\n",
        "            with Image.open(img_path) as img:\n",
        "                img = img.convert(\"RGB\").resize(TAREGT_SIZE_TUPLE)\n",
        "                img_arr = np.array(img)\n",
        "                img_hash = imagehash.phash(img)\n",
        "            if img_hash not in subfolder_hashes.keys():\n",
        "                train_features_arrays.append({\"img_path\": img_path, \"label\": subfolder, \"img_arr\": img_arr})\n",
        "                # key : hash, value : img_path\n",
        "                subfolder_hashes[img_hash] = img_path\n",
        "            else:\n",
        "                existing_duplicate = subfolder_hashes[img_hash]\n",
        "                dups_pairs.add((img_path, existing_duplicate))\n",
        "            pbar.update(1)\n",
        "        \n",
        "df = pd.DataFrame(train_features_arrays)\n",
        "\n",
        "# display dups\n",
        "for dup_pair in dups_pairs:\n",
        "    print(f\"Duplicate images found: {dup_pair[0]} and {dup_pair[1]}\")\n",
        "    img1 = Image.open(dup_pair[0])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img1)\n",
        "    plt.title(os.path.basename(dup_pair[0]))\n",
        "    plt.axis('off')\n",
        "    img2 = Image.open(dup_pair[1])\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img2)\n",
        "    plt.title(os.path.basename(dup_pair[1]))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "calculate_and_print_time(global_start_time, \"Loading images into memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Test/Val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C_b_7bkU03Y",
        "outputId": "7f2ab7fd-d76a-42c0-ac0d-fee917e716d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 81\n",
            "Validation set size: 23\n",
            "Test set size: 12\n",
            "\n",
            "training set:\n",
            "label\n",
            "Happy      21\n",
            "Sad        20\n",
            "Neutral    20\n",
            "Angry      20\n",
            "Name: count, dtype: int64\n",
            "\n",
            "validation set:\n",
            "label\n",
            "Sad        6\n",
            "Neutral    6\n",
            "Angry      6\n",
            "Happy      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "test set:\n",
            "label\n",
            "Neutral    3\n",
            "Happy      3\n",
            "Sad        3\n",
            "Angry      3\n",
            "Name: count, dtype: int64\n",
            "done splitting dataset into train, val, test\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=(1 - train_ratio), stratify=df['label'], random_state=RANDOM_SEED)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=(test_ratio / (test_ratio + val_ratio)), stratify=temp_df['label'], random_state=RANDOM_SEED)\n",
        "\n",
        "# Print the sizes of each split\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "print(\"\\ntraining set:\")\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "print(\"\\nvalidation set:\")\n",
        "print(val_df['label'].value_counts())\n",
        "\n",
        "print(\"\\ntest set:\")\n",
        "print(test_df['label'].value_counts())\n",
        "\n",
        "\n",
        "print(\"done splitting dataset into train, val, test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgDAdbjXbtMy"
      },
      "source": [
        "## **Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classes & Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvLayer:\n",
        "    def __init__(self, filter_size=3, num_filters=5, filter_weights=None):\n",
        "        self.filter_size = filter_size\n",
        "        self.num_filters = num_filters\n",
        "        if filter_weights is None:\n",
        "            filter_weights = np.random.normal(size=(num_filters, filter_size, filter_size)) * 0.1\n",
        "        self.filter_weights = filter_weights\n",
        "\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "        self.deep_filters = None\n",
        "        self.deep_filters_shape = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        input_h, input_w, input_c = input_shape\n",
        "\n",
        "        # ------------------- define the output shape \n",
        "        output_h = input_h - self.filter_size + 1\n",
        "        output_w = input_w - self.filter_size + 1\n",
        "        self.output_shape = (output_h, output_w, self.num_filters)\n",
        "\n",
        "        # ------------------- initilize deep filters \n",
        "        #  extend the filter weights depth so that filter depth = input depth (input channels)\n",
        "        self.deep_filters = extend_depth(self.filter_weights, input_c)\n",
        "        self.deep_filters_shape = self.deep_filters.shape\n",
        "        # flatten the filters for fast matrix multiplication in the \"forward\" fuction\n",
        "        self.deep_filters = self.deep_filters.reshape(self.num_filters, -1)\n",
        "\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        output_h, output_w = self.output_shape[0], self.output_shape[1]\n",
        "        input_col = get_image_flattened_windows(input_image, self.filter_size)\n",
        "        output_flat = self.deep_filters @ input_col.T\n",
        "        output = output_flat.T.reshape(output_h, output_w, self.num_filters)\n",
        "        return output\n",
        "    \n",
        "\n",
        "class PoolingLayer:\n",
        "    def __init__(self, pool_size=2, pool_type='MAX'):\n",
        "        if pool_type not in ['MAX', 'AVERAGE']:\n",
        "            raise ValueError(\"pool_type must be either 'MAX' or 'AVERAGE'\")\n",
        "        self.pool_size = pool_size\n",
        "        self.pool_type = pool_type\n",
        "\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "        self.output = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        input_h, input_w, input_c = input_shape\n",
        "\n",
        "        # define the output shape\n",
        "        output_h = input_h // self.pool_size\n",
        "        output_w = input_w // self.pool_size\n",
        "        self.output_shape = (output_h, output_w, input_c)\n",
        "\n",
        "        # initialize the output array\n",
        "        self.output = np.zeros(self.output_shape)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        for i in range(self.output.shape[0]):\n",
        "            for j in range(self.output.shape[1]):\n",
        "                h_start = i * self.pool_size\n",
        "                h_end = h_start + self.pool_size\n",
        "                w_start = j * self.pool_size\n",
        "                w_end = w_start + self.pool_size\n",
        "\n",
        "                pool_window = input_image[h_start:h_end, w_start:w_end, :]\n",
        "\n",
        "                if self.pool_type == 'MAX':\n",
        "                    self.output[i, j, :] = np.max(pool_window, axis=(0, 1))\n",
        "                elif self.pool_type == 'AVERAGE':\n",
        "                    self.output[i, j, :] = np.mean(pool_window, axis=(0, 1))\n",
        "\n",
        "        # apply relu\n",
        "        return np.maximum(0, self.output)\n",
        "    \n",
        "class FlatteningLayer:\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = (input_shape[0] * input_shape[1] * input_shape[2],)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        return input_image.flatten()\n",
        "\n",
        "\n",
        "class Model_1:\n",
        "    def __init__(self, layers, first_layer_input_shape):\n",
        "        self.layers = layers\n",
        "\n",
        "        # Set the input shape for each layer based on the output shape of the previous layer\n",
        "        previous_layer_shape = first_layer_input_shape\n",
        "        for layer in self.layers:\n",
        "            layer.set_input_shape(previous_layer_shape)\n",
        "            previous_layer_shape = layer.output_shape  \n",
        "\n",
        "    def visualize_architecture(self):\n",
        "        print(\"Model architecture : 🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️\\n\")\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(f\"Layer {i + 1}: {layer.__class__.__name__}\")\n",
        "            print(f\"Input shape: {layer.input_shape}\")\n",
        "            print(f\"Output shape: {layer.output_shape}\")\n",
        "            if isinstance(layer, ConvLayer):\n",
        "                print(f\"Number of filters: {layer.deep_filters_shape[0]}\")\n",
        "                print(f\"Filter Shape: {layer.deep_filters_shape[-3:]}\")\n",
        "            elif isinstance(layer, PoolingLayer):\n",
        "                print(f\"Pooling size: {layer.pool_size}\")\n",
        "                print(f\"Pooling type: {layer.pool_type}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        output = input_image\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "class Custom_Kmeans:\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "        self.centroids = None\n",
        "        self.clusters_names = None\n",
        "        self.index_labels = None\n",
        "\n",
        "    def predict(self, feature_arr):\n",
        "        distances = np.linalg.norm(feature_arr - self.centroids, axis=1)\n",
        "        cluster_index = np.argmin(distances)\n",
        "\n",
        "        # replace every index label with the name of the centroid, put in a new array\n",
        "        # exmaple : convert [0, 1, 2, 3] to ['happy', 'sad', 'angry', 'neutral']\n",
        "        return self.clusters_names[cluster_index]\n",
        "    \n",
        "    def get_clusters_names(self, true_labels):\n",
        "        num_of_classes = len(np.unique(true_labels))\n",
        "        cluster_names = []\n",
        "        for i in range(num_of_classes):\n",
        "            cluster_labels = true_labels[self.index_labels == i]\n",
        "            most_common_label = pd.Series(cluster_labels).mode()[0]\n",
        "            cluster_names.append(most_common_label)\n",
        "        return np.array(cluster_names)\n",
        "    \n",
        "    def fit(self, train_features_arrays, train_features_arrays_labels, val_features_arrays, val_features_arrays_labels):\n",
        "        # features_arrays is a 2D array of shape (num_images, num_features)\n",
        "        # features_arrays_labels is a 1D array of shape (num_samples,), \n",
        "        # representing the labels of images whose features are passed as features_arrays\n",
        "        rows_count = train_features_arrays.shape[0]\n",
        "\n",
        "        # Initialize centroids randomly    gg\n",
        "        self.centroids = train_features_arrays[np.random.choice(rows_count, self.k, replace=False)]\n",
        "\n",
        "        iterations_accuracies = []\n",
        "        while True:\n",
        "            distances = np.linalg.norm(train_features_arrays[:, np.newaxis] - self.centroids, axis=2)\n",
        "            self.index_labels = np.argmin(distances, axis=1) \n",
        "            previous_centroids = self.centroids.copy()\n",
        "            self.centroids = np.array([train_features_arrays[self.index_labels == i].mean(axis=0) for i in range(self.k)])\n",
        "            \n",
        "            # name each cluster as the name of the most frequent label in the cluster\n",
        "            self.clusters_names = self.get_clusters_names(train_features_arrays_labels)\n",
        "\n",
        "            # predict the labels of the validation set\n",
        "            val_predicted_labels = np.array([self.predict(val_features_arrays[i]) for i in range(len(val_features_arrays))])\n",
        "\n",
        "            # calculate the accuracy\n",
        "            val_accuracy = calculate_accuracy(val_features_arrays_labels, val_predicted_labels)\n",
        "\n",
        "            iterations_accuracies.append(val_accuracy)\n",
        "\n",
        "            # breack if difference between new and old centroids is small\n",
        "            tolerance = 1e-4\n",
        "            if np.all(np.linalg.norm(self.centroids - previous_centroids, axis=1) < tolerance):\n",
        "                break\n",
        "        return iterations_accuracies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the model to extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model architecture : 🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️🛠️\n",
            "\n",
            "Layer 1: ConvLayer\n",
            "Input shape: (512, 512, 3)\n",
            "Output shape: (510, 510, 5)\n",
            "Number of filters: 5\n",
            "Filter Shape: (3, 3, 3)\n",
            "------------------------------\n",
            "Layer 2: PoolingLayer\n",
            "Input shape: (510, 510, 5)\n",
            "Output shape: (255, 255, 5)\n",
            "Pooling size: 2\n",
            "Pooling type: MAX\n",
            "------------------------------\n",
            "Layer 3: ConvLayer\n",
            "Input shape: (255, 255, 5)\n",
            "Output shape: (253, 253, 5)\n",
            "Number of filters: 5\n",
            "Filter Shape: (3, 3, 5)\n",
            "------------------------------\n",
            "Layer 4: PoolingLayer\n",
            "Input shape: (253, 253, 5)\n",
            "Output shape: (126, 126, 5)\n",
            "Pooling size: 2\n",
            "Pooling type: MAX\n",
            "------------------------------\n",
            "Layer 5: FlatteningLayer\n",
            "Input shape: (126, 126, 5)\n",
            "Output shape: (79380,)\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features...: 100%|██████████| 81/81 [01:08<00:00,  1.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Extraction without downsampling Done in : 0 h, 1 m, 8 s\n",
            "selecting random 128 features from 81 features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features...: 100%|██████████| 23/23 [00:19<00:00,  1.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Extraction without downsampling Done in : 0 h, 0 m, 19 s\n",
            "selecting random 128 features from 23 features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "filter_weights = np.stack([\n",
        "    np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]),\n",
        "    np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),\n",
        "    np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n",
        "    np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),\n",
        "    np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "], axis=0)\n",
        "\n",
        "model_1 = Model_1(\n",
        "    layers=[\n",
        "        ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights),\n",
        "        PoolingLayer(pool_size=2, pool_type='MAX'),\n",
        "        ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights),\n",
        "        PoolingLayer(pool_size=2, pool_type='MAX'),\n",
        "        FlatteningLayer()\n",
        "    ],\n",
        "    first_layer_input_shape=df.iloc[0]['img_arr'].shape\n",
        ")\n",
        "model_1.visualize_architecture()\n",
        "\n",
        "\n",
        "train_features_arrays = extract_features(model_1, train_df)\n",
        "val_features_arrays = extract_features(model_1, val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 1 val accuracy:  0.21739130434782608\n",
            "Cluster 0:\n",
            "Neutral    8\n",
            "Angry      6\n",
            "Sad        5\n",
            "Happy      3\n",
            "Name: count, dtype: int64\n",
            "------------------------------\n",
            "Cluster 1:\n",
            "Sad        9\n",
            "Happy      7\n",
            "Angry      5\n",
            "Neutral    2\n",
            "Name: count, dtype: int64\n",
            "------------------------------\n",
            "Cluster 2:\n",
            "Happy      4\n",
            "Neutral    4\n",
            "Angry      3\n",
            "Sad        3\n",
            "Name: count, dtype: int64\n",
            "------------------------------\n",
            "Cluster 3:\n",
            "Happy      7\n",
            "Angry      6\n",
            "Neutral    6\n",
            "Sad        3\n",
            "Name: count, dtype: int64\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(df['label'].unique())\n",
        "train_features_arrays_labels = train_df['label'].values\n",
        "val_features_arrays_labels = val_df['label'].values\n",
        "\n",
        "kmeans = Custom_Kmeans(k=num_classes)\n",
        "clustering_val_accuracies_histroy = kmeans.fit(train_features_arrays, train_features_arrays_labels, val_features_arrays, val_features_arrays_labels)\n",
        "print(\"Model 1 val accuracy: \", clustering_val_accuracies_histroy[-1])\n",
        "\n",
        "# print the number of each class in each cluster\n",
        "for i in range(num_classes):\n",
        "    cluster_labels = train_features_arrays_labels[kmeans.index_labels == i]\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(pd.Series(cluster_labels).value_counts())\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing model 1 on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features...: 100%|██████████| 12/12 [00:10<00:00,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Extraction without downsampling Done in : 0 h, 0 m, 10 s\n",
            "selecting random 128 features from 12 features\n",
            "Model 1 test accuracy:  0.25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_features_arrays = extract_features(model_1, test_df)\n",
        "test_features_arrays_labels = test_df['label'].values\n",
        "test_predicted_labels = np.array([kmeans.predict(test_features_arrays[i]) for i in range(len(test_features_arrays))])\n",
        "test_accuracy = calculate_accuracy(test_features_arrays_labels, test_predicted_labels)\n",
        "print(\"Model 1 test accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7VknR2cbydf"
      },
      "source": [
        "## **Model 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "xFDVzn_qb359",
        "outputId": "a0c234e1-4fc7-4dfb-f08e-32af3e76f40a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done building model 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hosain/Desktop/DL project/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# TA Dinah : \"You should have 5 convolution blocks each with 3 layers.\n",
        "#  3 convolution layers are simply 3 different filters on the same stage.\n",
        "#  A convolution block is some conv filters (layers) followed by an activation function and then a max pooling. \n",
        "# All convolution filters in the same block need to have the same size.\"\n",
        "\n",
        "def create_model2():\n",
        "    model = models.Sequential([\n",
        "    \n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid', input_shape=(TAREGT_SIZE_TUPLE[0], TAREGT_SIZE_TUPLE[1], 3)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    # 128 is OUR CHOICE for the number of neurons in the hidden layer (not specified in the project description)\n",
        "    layers.Dense(128, activation='sigmoid'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model2 = create_model2()\n",
        "\n",
        "\n",
        "print(\"done building model 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Early stopping to prevent overfitting (for the BONUS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                            patience=2,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr = 0.00001,\n",
        "                                            verbose = 1)\n",
        "\n",
        "early_stoping = EarlyStopping(monitor='val_loss',patience= 3,restore_best_weights=True,verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data augmentation (for the BONUS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [],
      "source": [
        "# thiss will rotate the images by up to 20 degrees, also will increase the \n",
        "# dataset size on the fly while training \n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4GKqc0Yfl5e",
        "outputId": "7862dc2e-3b93-4508-d04a-01be54e7f565"
      },
      "outputs": [],
      "source": [
        "# start_time = time()\n",
        "# batch_size = 16\n",
        "# epochs = 10\n",
        "\n",
        "# # print trining dataset size before and after augmentation\n",
        "# print(f\"Training dataset size before augmentation: {len(train_df)}\")\n",
        "# train_gen = train_datagen.flow_from_dataframe(\n",
        "#     dataframe=train_df,\n",
        "#     target_size=TAREGT_SIZE_TUPLE,\n",
        "#     x_col='img_path',  \n",
        "#     y_col='label',    \n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical', \n",
        "#     shuffle=True\n",
        "# )\n",
        "# print(f\"Training dataset size after augmentation: {train_gen.n}\")\n",
        "\n",
        "# val_gen = val_datagen.flow_from_dataframe(\n",
        "#     dataframe=val_df,\n",
        "#     target_size=TAREGT_SIZE_TUPLE,\n",
        "#     x_col='img_path',\n",
        "#     y_col='label',\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical',\n",
        "#     shuffle=False\n",
        "# )\n",
        "# print(f\"images sizes passed to image data generator: {train_gen.image_shape}\")\n",
        "\n",
        "# test_gen = test_datagen.flow_from_dataframe(\n",
        "#     dataframe=test_df,\n",
        "#     target_size=TAREGT_SIZE_TUPLE,\n",
        "#     x_col='img_path',\n",
        "#     y_col='label',\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical',\n",
        "#     shuffle=False\n",
        "# )\n",
        "\n",
        "\n",
        "# model2_history = model2.fit(\n",
        "#     train_gen,\n",
        "#     validation_data=val_gen,\n",
        "#     callbacks=[early_stoping, learning_rate_reduction],\n",
        "#     epochs=epochs,\n",
        "# )\n",
        "\n",
        "# test_loss, test_acc = model2.evaluate(test_gen)\n",
        "# print(f\"Val accuracy: {model2_history.history['val_accuracy'][-1]:.2f}\")\n",
        "# print(f\"Test accuracy: {test_acc:.2f}\")\n",
        "\n",
        "\n",
        "# calculate_and_print_time(start_time, \"Training model 2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Displyaing samples inputs and outputs from both model 1 & model 2 (for the report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 1**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "JMOPXDv7hYqe",
        "outputId": "f4a6c5ed-9f4e-4057-9374-fa663d72add1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Milestone 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 1**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy vs iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot accuracy vs iterations\n",
        "plt.plot(clustering_val_accuracies_histroy)\n",
        "plt.plot(clustering_val_accuracies_histroy, marker='o', markersize=8, linewidth=2)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('KMeans Accuracy vs Iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 2**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy vs iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.plot(model2_history.history['accuracy'], label='train accuracy')\n",
        "# plt.plot(model2_history.history['val_accuracy'], label='val accuracy')\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [],
      "source": [
        "# start_time = time()\n",
        "# k = 4\n",
        "# kf = KFold(n_splits=k, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "# fold_accuracies = []\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "#     print(f\"\\nStarting fold {fold + 1}/{k}...\")\n",
        "    \n",
        "#     train_data = df.iloc[train_idx]\n",
        "#     val_data = df.iloc[val_idx]\n",
        "    \n",
        "#     train_gen = train_datagen.flow_from_dataframe(\n",
        "#         dataframe=train_data,\n",
        "#         x_col='img_path',\n",
        "#         y_col='label',\n",
        "#         target_size=TAREGT_SIZE_TUPLE,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode='categorical',\n",
        "#         shuffle=True\n",
        "#     )\n",
        "    \n",
        "#     val_gen = val_datagen.flow_from_dataframe(\n",
        "#         dataframe=val_data,\n",
        "#         x_col='img_path',\n",
        "#         y_col='label',\n",
        "#         target_size=TAREGT_SIZE_TUPLE,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode='categorical',\n",
        "#         shuffle=False\n",
        "#     )\n",
        "    \n",
        "#     model2 = create_model2()\n",
        "    \n",
        "#     model2_history = model2.fit(\n",
        "#         train_gen,\n",
        "#         validation_data=val_gen,\n",
        "#         callbacks=[early_stoping, learning_rate_reduction],\n",
        "#         epochs=epochs,\n",
        "#     )\n",
        "    \n",
        "#     val_accuracy = model2.evaluate(val_gen)[1]\n",
        "#     print(f\"\\nFold {fold + 1} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "#     fold_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "# average_accuracy = sum(fold_accuracies) / k\n",
        "# print(f\"All folds validation accuracies: {fold_accuracies}\")\n",
        "# print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
        "\n",
        "\n",
        "# calculate_and_print_time(start_time, \"Model 2 K-fold cross-validation\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def display_confusion_matrix(model, val_gen, plt_title):\n",
        "#     y_true = val_gen.classes\n",
        "#     print(f\"y_true : {y_true}\")\n",
        "#     y_pred = model.predict(val_gen)\n",
        "#     # print(f\"y_pred : {y_pred}\")\n",
        "#     y_pred_classes = y_pred.argmax(axis=1)\n",
        "    \n",
        "#     cm = confusion_matrix(y_true, y_pred_classes)\n",
        "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_gen.class_indices.keys())\n",
        "#     disp.plot(cmap='viridis')\n",
        "#     plt.title(plt_title)\n",
        "#     plt.show()\n",
        "\n",
        "# display_confusion_matrix(model2, val_gen, \"Confusion Matrix for model 2\")\n",
        "# calculate_and_print_time(global_start_time, \"All Notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### notes : model2 problem is not in the model architecture (varified this by trying on bachelor model architecture, got similar results) so the problem is most likely in the data, or some where else in the code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
