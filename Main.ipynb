{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "\n",
        "report both models results on some samples from test data\n",
        "<br>\n",
        "\n",
        "modify dataset perc to use\n",
        "\n",
        "Train/test timing\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Milestone 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Packages installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in ./.venv/lib/python3.12/site-packages (0.3.11)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (2025.1.31)\n",
            "Requirement already satisfied: imagehash in ./.venv/lib/python3.12/site-packages (4.3.2)\n",
            "Requirement already satisfied: PyWavelets in ./.venv/lib/python3.12/site-packages (from imagehash) (1.8.0)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from imagehash) (2.1.3)\n",
            "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (from imagehash) (11.2.1)\n",
            "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from imagehash) (1.15.2)\n",
            "Requirement already satisfied: tensorflow in ./.venv/lib/python3.12/site-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in ./.venv/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.venv/lib/python3.12/site-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from tensorflow) (78.1.1)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.67.1)\n",
            "done installing packages\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub\n",
        "!pip install imagehash\n",
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "print(\"done installing packages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done importing packages\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import random\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.decomposition import PCA\n",
        "from time import time\n",
        "\n",
        "print(\"done importing packages\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Data preparaion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done setting up dataset path\n"
          ]
        }
      ],
      "source": [
        "if 'COLAB_GPU' in os.environ:\n",
        "    dataset_folder = kagglehub.dataset_download('hussainghoraba/emotions-dataset')\n",
        "    DATASET_PATH = os.path.join(dataset_folder, 'Dataset')\n",
        "elif 'KAGGLE_URL_BASE' in os.environ:\n",
        "    DATASET_PATH = '/kaggle/input/emotions-dataset/Dataset'\n",
        "elif 'VSCODE_PID' in os.environ:\n",
        "    DATASET_PATH = './Dataset'\n",
        "else:\n",
        "    raise Exception('Unknown environment')\n",
        "\n",
        "print(\"done setting up dataset path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set random seed & some global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done setting up random seed\n"
          ]
        }
      ],
      "source": [
        "RANDOM_SEED = 42\n",
        "TAREGT_SIZE_TUPLE = (512, 512)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
        "print(\"done setting up random seed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_and_print_time(start_time, section_name):\n",
        "    end_time = time()\n",
        "    time_taken = end_time - start_time\n",
        "    hours = int(time_taken // 3600)\n",
        "    minutes = int((time_taken % 3600) // 60)\n",
        "    seconds = int((time_taken % 3600) % 60)\n",
        "    print(f'\\n{section_name} Done in : {hours} h, {minutes} m, {seconds} s')\n",
        "    \n",
        "def extend_depth(filters, depth):\n",
        "    return np.stack([filters] * depth, axis=-1)\n",
        "\n",
        "def get_image_flattened_windows(input_data, filter_size):\n",
        "    input_h, input_w, input_c = input_data.shape\n",
        "    output_h = input_h - filter_size + 1\n",
        "    output_w = input_w - filter_size + 1\n",
        "\n",
        "    col = np.zeros((output_h * output_w, filter_size * filter_size * input_c))\n",
        "    i = 0\n",
        "    for h in range(output_h):\n",
        "        for w in range(output_w):\n",
        "            cube = input_data[h:h+filter_size, w:w+filter_size, :]\n",
        "            col[i, :] = cube.flatten()\n",
        "            i += 1\n",
        "    return col\n",
        "\n",
        "def custom_kmeans(data, k):\n",
        "    start_time = time()\n",
        "    # data is a 2d array, each row is an array containing \n",
        "    # the extracted features of images passed to model 1\n",
        "\n",
        "    rows_count = data.shape[0]\n",
        "\n",
        "    # Initialize centroids randomly\n",
        "    centroids = data[np.random.choice(rows_count, k, replace=False)]\n",
        "\n",
        "    while True:\n",
        "        # assign each sample to the nearest centroid\n",
        "        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
        "        labels = np.argmin(distances, axis=1) \n",
        "\n",
        "        # update centroids \n",
        "        new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "        # breack if difference between new and old centroids is small\n",
        "        tolerance = 1e-4\n",
        "        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) < tolerance):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    calculate_and_print_time(start_time, \"Custom KMeans\")\n",
        "    return centroids, labels\n",
        "\n",
        "def downsample_features(features_arrays):\n",
        "    # downsample the features arrays to only 128 dimensions\n",
        "    # use PCA only if we have more than 128 feature arrays ,\n",
        "    # this is because in PCA, the number of components (which is 128 in our case) \n",
        "    # must be less than the number of samples.\n",
        "    # if we have less than 128 feature arrays, we will just select random 128 features\n",
        "    start_time = time()\n",
        "    if features_arrays.shape[0] >= 128:\n",
        "        print(f\"appling PCA...\")\n",
        "        features_arrays = PCA(n_components=128).fit_transform(features_arrays)\n",
        "    else:\n",
        "        print(f\"selecting random 128 features from {features_arrays.shape[0]} features\")\n",
        "        selected_indices = np.random.choice(features_arrays.shape[1], 128, replace=False)\n",
        "        features_arrays = features_arrays[:, selected_indices]\n",
        "    calculate_and_print_time(start_time, \"Downsampling features\")\n",
        "    return features_arrays\n",
        "\n",
        "def extract_features(model, df):\n",
        "    start_time = time()\n",
        "    num_features = model.layers[-1].output_shape[0]\n",
        "    features_arrays = np.zeros((len(df), num_features))\n",
        "\n",
        "    with tqdm(total=len(df), desc=\"Extracting features...\") as pbar:\n",
        "        for i in range(len(df)):\n",
        "            img_arr = df.iloc[i]['img_arr']\n",
        "            features_arrays[i, :] = model.forward(img_arr)\n",
        "            pbar.update(1)\n",
        "    calculate_and_print_time(start_time, \"Feature Extraction without downsampling\")\n",
        "    return downsample_features(features_arrays)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset into memory without dups, and with correct size, and equalize the number of images in each class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images in dataset: 2125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading images into memory...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 116/118 [00:01<00:00, 81.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading images into memory Done in : 0 h, 0 m, 1 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "global_start_time = time()\n",
        "total_images_count = sum(len(files) for _, _, files in os.walk(DATASET_PATH))\n",
        "print(f\"Total images in dataset: {total_images_count}\")\n",
        "data = []\n",
        "dups_pairs = set()\n",
        "\n",
        "# load only a small percentage of the dataset, for faster testing while developing\n",
        "DATASET_PERC_TO_USE = 0.1\n",
        "\n",
        "num_images_in_smallest_category = min(len(os.listdir(os.path.join(DATASET_PATH, folder))) for folder in os.listdir(DATASET_PATH))\n",
        "num_of_images_to_use_in_each_category = int(num_images_in_smallest_category * DATASET_PERC_TO_USE)\n",
        "num_of_categories = len(os.listdir(DATASET_PATH))\n",
        "total_images_to_load = int(num_images_in_smallest_category * num_of_categories * DATASET_PERC_TO_USE)\n",
        "\n",
        "with tqdm(total=total_images_to_load, desc=\"Loading images into memory...\") as pbar:\n",
        "    for subfolder in os.listdir(DATASET_PATH):\n",
        "        subfolder_path = os.path.join(DATASET_PATH, subfolder)\n",
        "        subfolder_hashes = {}\n",
        "\n",
        "        all_category_images = os.listdir(subfolder_path)\n",
        "        # we must use the same number of images from each category to avoid bias\n",
        "        images_to_load = random.sample(all_category_images, num_of_images_to_use_in_each_category)\n",
        "        \n",
        "        for img_file in images_to_load:\n",
        "            img_path = os.path.join(subfolder_path, img_file)\n",
        "            with Image.open(img_path) as img:\n",
        "                img = img.convert(\"RGB\").resize(TAREGT_SIZE_TUPLE)\n",
        "                img_arr = np.array(img)\n",
        "                img_hash = imagehash.phash(img)\n",
        "            if img_hash not in subfolder_hashes.keys():\n",
        "                data.append({\"img_path\": img_path, \"label\": subfolder, \"img_arr\": img_arr})\n",
        "                # key : hash, value : img_path\n",
        "                subfolder_hashes[img_hash] = img_path\n",
        "            else:\n",
        "                existing_duplicate = subfolder_hashes[img_hash]\n",
        "                dups_pairs.add((img_path, existing_duplicate))\n",
        "            pbar.update(1)\n",
        "        \n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# display dups\n",
        "for dup_pair in dups_pairs:\n",
        "    print(f\"Duplicate images found: {dup_pair[0]} and {dup_pair[1]}\")\n",
        "    img1 = Image.open(dup_pair[0])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img1)\n",
        "    plt.title(os.path.basename(dup_pair[0]))\n",
        "    plt.axis('off')\n",
        "    img2 = Image.open(dup_pair[1])\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img2)\n",
        "    plt.title(os.path.basename(dup_pair[1]))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "calculate_and_print_time(global_start_time, \"Loading images into memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Test/Val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C_b_7bkU03Y",
        "outputId": "7f2ab7fd-d76a-42c0-ac0d-fee917e716d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 81\n",
            "Validation set size: 23\n",
            "Test set size: 12\n",
            "\n",
            "training set:\n",
            "label\n",
            "Happy      21\n",
            "Sad        20\n",
            "Neutral    20\n",
            "Angry      20\n",
            "Name: count, dtype: int64\n",
            "\n",
            "validation set:\n",
            "label\n",
            "Sad        6\n",
            "Neutral    6\n",
            "Angry      6\n",
            "Happy      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "test set:\n",
            "label\n",
            "Neutral    3\n",
            "Happy      3\n",
            "Sad        3\n",
            "Angry      3\n",
            "Name: count, dtype: int64\n",
            "done splitting dataset into train, val, test\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=(1 - train_ratio), stratify=df['label'], random_state=RANDOM_SEED)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=(test_ratio / (test_ratio + val_ratio)), stratify=temp_df['label'], random_state=RANDOM_SEED)\n",
        "\n",
        "# Print the sizes of each split\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "print(\"\\ntraining set:\")\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "print(\"\\nvalidation set:\")\n",
        "print(val_df['label'].value_counts())\n",
        "\n",
        "print(\"\\ntest set:\")\n",
        "print(test_df['label'].value_counts())\n",
        "\n",
        "\n",
        "print(\"done splitting dataset into train, val, test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgDAdbjXbtMy"
      },
      "source": [
        "## **Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classes & Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.pylab import f\n",
        "\n",
        "\n",
        "class ConvLayer:\n",
        "    def __init__(self, filter_size=3, num_filters=5, filter_weights=None):\n",
        "        self.filter_size = filter_size\n",
        "        self.num_filters = num_filters\n",
        "        if filter_weights is None:\n",
        "            filter_weights = np.random.normal(size=(num_filters, filter_size, filter_size)) * 0.1\n",
        "        self.filter_weights = filter_weights\n",
        "\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "        self.deep_filters = None\n",
        "        self.deep_filters_shape = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        input_h, input_w, input_c = input_shape\n",
        "\n",
        "        # ------------------- define the output shape \n",
        "        output_h = input_h - self.filter_size + 1\n",
        "        output_w = input_w - self.filter_size + 1\n",
        "        self.output_shape = (output_h, output_w, self.num_filters)\n",
        "\n",
        "        # ------------------- initilize deep filters \n",
        "        #  extend the filter weights depth so that filter depth = input depth (input channels)\n",
        "        self.deep_filters = extend_depth(self.filter_weights, input_c)\n",
        "        self.deep_filters_shape = self.deep_filters.shape\n",
        "        # flatten the filters for fast matrix multiplication in the \"forward\" fuction\n",
        "        self.deep_filters = self.deep_filters.reshape(self.num_filters, -1)\n",
        "\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        output_h, output_w = self.output_shape[0], self.output_shape[1]\n",
        "        input_col = get_image_flattened_windows(input_image, self.filter_size)\n",
        "        output_flat = self.deep_filters @ input_col.T\n",
        "        output = output_flat.T.reshape(output_h, output_w, self.num_filters)\n",
        "        return output\n",
        "    \n",
        "\n",
        "class PoolingLayer:\n",
        "    def __init__(self, pool_size=2, pool_type='MAX'):\n",
        "        if pool_type not in ['MAX', 'AVERAGE']:\n",
        "            raise ValueError(\"pool_type must be either 'MAX' or 'AVERAGE'\")\n",
        "        self.pool_size = pool_size\n",
        "        self.pool_type = pool_type\n",
        "\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "        self.output = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        input_h, input_w, input_c = input_shape\n",
        "\n",
        "        # define the output shape\n",
        "        output_h = input_h // self.pool_size\n",
        "        output_w = input_w // self.pool_size\n",
        "        self.output_shape = (output_h, output_w, input_c)\n",
        "\n",
        "        # initialize the output array\n",
        "        self.output = np.zeros(self.output_shape)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        for i in range(self.output.shape[0]):\n",
        "            for j in range(self.output.shape[1]):\n",
        "                h_start = i * self.pool_size\n",
        "                h_end = h_start + self.pool_size\n",
        "                w_start = j * self.pool_size\n",
        "                w_end = w_start + self.pool_size\n",
        "\n",
        "                pool_window = input_image[h_start:h_end, w_start:w_end, :]\n",
        "\n",
        "                if self.pool_type == 'MAX':\n",
        "                    self.output[i, j, :] = np.max(pool_window, axis=(0, 1))\n",
        "                elif self.pool_type == 'AVERAGE':\n",
        "                    self.output[i, j, :] = np.mean(pool_window, axis=(0, 1))\n",
        "\n",
        "        # apply relu\n",
        "        return np.maximum(0, self.output)\n",
        "    \n",
        "class FlatteningLayer:\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "        self.output_shape = None\n",
        "\n",
        "    def set_input_shape(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = (input_shape[0] * input_shape[1] * input_shape[2],)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        return input_image.flatten()\n",
        "\n",
        "\n",
        "class Model_1:\n",
        "    def __init__(self, layers, first_layer_input_shape):\n",
        "        self.layers = layers\n",
        "\n",
        "        # Set the input shape for each layer based on the output shape of the previous layer\n",
        "        previous_layer_shape = first_layer_input_shape\n",
        "        for layer in self.layers:\n",
        "            layer.set_input_shape(previous_layer_shape)\n",
        "            previous_layer_shape = layer.output_shape  \n",
        "\n",
        "    def visualize_architecture(self):\n",
        "        print(\"Model architecture : ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸\\n\")\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(f\"Layer {i + 1}: {layer.__class__.__name__}\")\n",
        "            print(f\"Input shape: {layer.input_shape}\")\n",
        "            print(f\"Output shape: {layer.output_shape}\")\n",
        "            if isinstance(layer, ConvLayer):\n",
        "                print(f\"Number of filters: {layer.deep_filters_shape[0]}\")\n",
        "                print(f\"Filter Shape: {layer.deep_filters_shape[-3:]}\")\n",
        "            elif isinstance(layer, PoolingLayer):\n",
        "                print(f\"Pooling size: {layer.pool_size}\")\n",
        "                print(f\"Pooling type: {layer.pool_type}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        output = input_image\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the model to extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model architecture : ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸ðŸ› ï¸\n",
            "\n",
            "Layer 1: ConvLayer\n",
            "Input shape: (512, 512, 3)\n",
            "Output shape: (510, 510, 5)\n",
            "Number of filters: 5\n",
            "Filter Shape: (3, 3, 3)\n",
            "------------------------------\n",
            "Layer 2: PoolingLayer\n",
            "Input shape: (510, 510, 5)\n",
            "Output shape: (255, 255, 5)\n",
            "Pooling size: 2\n",
            "Pooling type: MAX\n",
            "------------------------------\n",
            "Layer 3: ConvLayer\n",
            "Input shape: (255, 255, 5)\n",
            "Output shape: (253, 253, 5)\n",
            "Number of filters: 5\n",
            "Filter Shape: (3, 3, 5)\n",
            "------------------------------\n",
            "Layer 4: PoolingLayer\n",
            "Input shape: (253, 253, 5)\n",
            "Output shape: (126, 126, 5)\n",
            "Pooling size: 2\n",
            "Pooling type: MAX\n",
            "------------------------------\n",
            "Layer 5: FlatteningLayer\n",
            "Input shape: (126, 126, 5)\n",
            "Output shape: (79380,)\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features...:   5%|â–         | 4/81 [00:04<01:19,  1.03s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      9\u001b[39m model_1 = Model_1(\n\u001b[32m     10\u001b[39m     layers=[\n\u001b[32m     11\u001b[39m         ConvLayer(filter_size=\u001b[32m3\u001b[39m, num_filters=\u001b[32m5\u001b[39m, filter_weights=filter_weights),\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     first_layer_input_shape=df.iloc[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mimg_arr\u001b[39m\u001b[33m'\u001b[39m].shape\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m model_1.visualize_architecture()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m features_arrays = \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(model, df)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[32m     78\u001b[39m         img_arr = df.iloc[i][\u001b[33m'\u001b[39m\u001b[33mimg_arr\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m         features_arrays[i, :] = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m         pbar.update(\u001b[32m1\u001b[39m)\n\u001b[32m     81\u001b[39m calculate_and_print_time(start_time, \u001b[33m\"\u001b[39m\u001b[33mFeature Extraction without downsampling\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mModel_1.forward\u001b[39m\u001b[34m(self, input_image)\u001b[39m\n\u001b[32m    121\u001b[39m output = input_image\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     output = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mPoolingLayer.forward\u001b[39m\u001b[34m(self, input_image)\u001b[39m\n\u001b[32m     73\u001b[39m pool_window = input_image[h_start:h_end, w_start:w_end, :]\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool_type == \u001b[33m'\u001b[39m\u001b[33mMAX\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28mself\u001b[39m.output[i, j, :] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool_type == \u001b[33m'\u001b[39m\u001b[33mAVERAGE\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.output[i, j, :] = np.mean(pool_window, axis=(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL project/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3199\u001b[39m, in \u001b[36mmax\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3080\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[32m   3081\u001b[39m \u001b[38;5;129m@set_module\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3082\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, initial=np._NoValue,\n\u001b[32m   3083\u001b[39m          where=np._NoValue):\n\u001b[32m   3084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3085\u001b[39m \u001b[33;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[32m   3086\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3197\u001b[39m \u001b[33;03m    5\u001b[39;00m\n\u001b[32m   3198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3200\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL project/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:69\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m         \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m         \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, **kwargs):\n\u001b[32m     70\u001b[39m     passkwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m     71\u001b[39m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue}\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu.ndarray:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "filter_weights = np.stack([\n",
        "    np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]),\n",
        "    np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),\n",
        "    np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n",
        "    np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),\n",
        "    np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "], axis=0)\n",
        "\n",
        "model_1 = Model_1(\n",
        "    layers=[\n",
        "        ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights),\n",
        "        PoolingLayer(pool_size=2, pool_type='MAX'),\n",
        "        ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights),\n",
        "        PoolingLayer(pool_size=2, pool_type='MAX'),\n",
        "        FlatteningLayer()\n",
        "    ],\n",
        "    first_layer_input_shape=df.iloc[0]['img_arr'].shape\n",
        ")\n",
        "model_1.visualize_architecture()\n",
        "\n",
        "\n",
        "features_arrays = extract_features(model_1, train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = len(df['label'].unique())\n",
        "centroids, labels = custom_kmeans(features_arrays, k= num_classes)\n",
        "\n",
        "print(\"centroids shape:\", centroids.shape)\n",
        "print(\"labels shape:\", labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Function to load and preprocess an image\n",
        "# def load_image(url=None, path=None, target_size=(512, 512)):\n",
        "#     \"\"\"Load an image from URL or local path and preprocess it.\"\"\"\n",
        "#     if url:\n",
        "#         response = requests.get(url)\n",
        "#         img = Image.open(BytesIO(response.content))\n",
        "#     elif path:\n",
        "#         img = Image.open(path)\n",
        "#     else:\n",
        "#         # Create a sample image if no source is provided\n",
        "#         img = Image.new('RGB', target_size, color=(73, 109, 137))\n",
        "\n",
        "#     # Resize the image\n",
        "#     img = img.resize(target_size)\n",
        "\n",
        "#     # Convert to numpy array\n",
        "#     img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "#     return img_array\n",
        "\n",
        "# # Test with a sample image (you can replace this URL with your own image)\n",
        "# # Using a sample image URL - replace with your own image or add code to load a local file\n",
        "# sample_image_url = \"https://images.wallpapersden.com/image/download/cat-green-eyed-muzzle_aWhnZ5SZmpqtpaSklGpmZ61qZmc.jpg\"\n",
        "# try:\n",
        "#     img = load_image(url=sample_image_url)\n",
        "# except:\n",
        "#     # If the URL doesn't work, create a simple test image\n",
        "#     img = np.zeros((512, 512, 3))\n",
        "#     # Add some shapes for testing\n",
        "#     img[100:400, 100:400, 0] = 1.0  # Red square\n",
        "#     img[200:300, 200:300, 1] = 1.0  # Green square inside red\n",
        "\n",
        "# Initialize a ConvLayer with the predefined filters\n",
        "# conv_layer = ConvLayer(filter_size=3, num_filters=5, filter_weights=filters_3d)\n",
        "\n",
        "# # Apply convolution\n",
        "# feature_maps = conv_layer.forward(img)\n",
        "\n",
        "# # Visualize the original image and the feature maps\n",
        "# plt.figure(figsize=(15, 8))\n",
        "\n",
        "# # Original image\n",
        "# plt.subplot(2, 3, 1)\n",
        "# plt.imshow(img)\n",
        "# plt.title('Original Image')\n",
        "# plt.axis('off')\n",
        "\n",
        "# # Feature maps\n",
        "# filter_names = ['Box Filter (a)', 'Identity Filter (b)', 'Sobel X (c)', 'Sobel Y (d)', 'Sharpening (e)']\n",
        "\n",
        "# for i in range(5):\n",
        "#     plt.subplot(2, 3, i+2)\n",
        "#     # Normalize the feature map for better visualization\n",
        "#     feature_map = feature_maps[:, :, i]\n",
        "#     feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-8)\n",
        "#     plt.imshow(feature_map, cmap='viridis')\n",
        "#     plt.title(f'Filter {i+1}: {filter_names[i]}')\n",
        "#     plt.axis('off')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # Test individual filters\n",
        "# print(\"\\nTesting individual filters:\")\n",
        "# for i, (filter_name, filter_3d) in enumerate(zip(filter_names, [filter_a_3d, filter_b_3d, filter_c_3d, filter_d_3d, filter_e_3d])):\n",
        "#     # Create a ConvLayer with a single filter\n",
        "#     single_filter = np.expand_dims(filter_3d, axis=0)\n",
        "#     conv_layer_single = ConvLayer(filter_size=3, num_filters=1, filter_weights=single_filter)\n",
        "\n",
        "#     # Apply convolution\n",
        "#     feature_map = conv_layer_single.forward(img)\n",
        "\n",
        "#     print(f\"Filter {i+1} ({filter_name}) applied successfully.\")\n",
        "\n",
        "print(\"done model 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7VknR2cbydf"
      },
      "source": [
        "## **Model 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "xFDVzn_qb359",
        "outputId": "a0c234e1-4fc7-4dfb-f08e-32af3e76f40a"
      },
      "outputs": [],
      "source": [
        "# TA Dinah : \"You should have 5 convolution blocks each with 3 layers.\n",
        "#  3 convolution layers are simply 3 different filters on the same stage.\n",
        "#  A convolution block is some conv filters (layers) followed by an activation function and then a max pooling. \n",
        "# All convolution filters in the same block need to have the same size.\"\n",
        "\n",
        "def create_model2():\n",
        "    model = models.Sequential([\n",
        "    \n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid', input_shape=(TAREGT_SIZE_TUPLE[0], TAREGT_SIZE_TUPLE[1], 3)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    # 128 is OUR CHOICE for the number of neurons in the hidden layer (not specified in the project description)\n",
        "    layers.Dense(128, activation='sigmoid'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model2 = create_model2()\n",
        "\n",
        "\n",
        "print(\"done building model 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Early stopping to prevent overfitting (for the BONUS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                            patience=2,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr = 0.00001,\n",
        "                                            verbose = 1)\n",
        "\n",
        "early_stoping = EarlyStopping(monitor='val_loss',patience= 3,restore_best_weights=True,verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data augmentation (for the BONUS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# thiss will rotate the images by up to 20 degrees, also will increase the \n",
        "# dataset size on the fly while training \n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4GKqc0Yfl5e",
        "outputId": "7862dc2e-3b93-4508-d04a-01be54e7f565"
      },
      "outputs": [],
      "source": [
        "start_time = time()\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "\n",
        "# print trining dataset size before and after augmentation\n",
        "print(f\"Training dataset size before augmentation: {len(train_df)}\")\n",
        "train_gen = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    target_size=TAREGT_SIZE_TUPLE,\n",
        "    x_col='img_path',  \n",
        "    y_col='label',    \n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', \n",
        "    shuffle=True\n",
        ")\n",
        "print(f\"Training dataset size after augmentation: {train_gen.n}\")\n",
        "\n",
        "val_gen = val_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    target_size=TAREGT_SIZE_TUPLE,\n",
        "    x_col='img_path',\n",
        "    y_col='label',\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "print(f\"images sizes passed to image data generator: {train_gen.image_shape}\")\n",
        "\n",
        "test_gen = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    target_size=TAREGT_SIZE_TUPLE,\n",
        "    x_col='img_path',\n",
        "    y_col='label',\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "model2_history = model2.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[early_stoping, learning_rate_reduction],\n",
        "    epochs=epochs,\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model2.evaluate(test_gen)\n",
        "print(f\"Val accuracy: {model2_history.history['val_accuracy'][-1]:.2f}\")\n",
        "print(f\"Test accuracy: {test_acc:.2f}\")\n",
        "\n",
        "\n",
        "calculate_and_print_time(start_time, \"Training model 2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "JMOPXDv7hYqe",
        "outputId": "f4a6c5ed-9f4e-4057-9374-fa663d72add1"
      },
      "outputs": [],
      "source": [
        "# # Get class indices and reverse mapping\n",
        "# class_indices = test_gen.class_indices\n",
        "# class_labels = list(class_indices.keys())\n",
        "\n",
        "# # Get predictions for the test set\n",
        "# preds = model.predict(test_gen)\n",
        "# predicted_classes = np.argmax(preds, axis=1)\n",
        "# true_classes = test_gen.classes\n",
        "# filenames = test_gen.filenames\n",
        "\n",
        "# # Display 10 random images with predictions\n",
        "# num_images = 10\n",
        "# indices = np.random.choice(len(filenames), num_images, replace=False)\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "\n",
        "# for i, idx in enumerate(indices):\n",
        "#     img_path = os.path.join(test_dir, filenames[idx])\n",
        "#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
        "#     img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "\n",
        "#     plt.subplot(2, 5, i + 1)\n",
        "#     plt.imshow(img_array)\n",
        "#     plt.axis('off')\n",
        "#     true_label = class_labels[true_classes[idx]]\n",
        "#     predicted_label = class_labels[predicted_classes[idx]]\n",
        "#     title_color = \"green\" if true_label == predicted_label else \"red\"\n",
        "#     plt.title(f\"True: {true_label}\\nPred: {predicted_label}\", color=title_color)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "print(\"done displaying predictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Milestone 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 1**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy vs iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 2**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy vs iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.plot(model2_history.history['accuracy'], label='train accuracy')\n",
        "# plt.plot(model2_history.history['val_accuracy'], label='val accuracy')\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# start_time = time()\n",
        "# k = 4\n",
        "# kf = KFold(n_splits=k, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "# fold_accuracies = []\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "#     print(f\"\\nStarting fold {fold + 1}/{k}...\")\n",
        "    \n",
        "#     train_data = df.iloc[train_idx]\n",
        "#     val_data = df.iloc[val_idx]\n",
        "    \n",
        "#     train_gen = train_datagen.flow_from_dataframe(\n",
        "#         dataframe=train_data,\n",
        "#         x_col='img_path',\n",
        "#         y_col='label',\n",
        "#         target_size=TAREGT_SIZE_TUPLE,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode='categorical',\n",
        "#         shuffle=True\n",
        "#     )\n",
        "    \n",
        "#     val_gen = val_datagen.flow_from_dataframe(\n",
        "#         dataframe=val_data,\n",
        "#         x_col='img_path',\n",
        "#         y_col='label',\n",
        "#         target_size=TAREGT_SIZE_TUPLE,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode='categorical',\n",
        "#         shuffle=False\n",
        "#     )\n",
        "    \n",
        "#     model2 = create_model2()\n",
        "    \n",
        "#     model2_history = model2.fit(\n",
        "#         train_gen,\n",
        "#         validation_data=val_gen,\n",
        "#         callbacks=[early_stoping, learning_rate_reduction],\n",
        "#         epochs=epochs,\n",
        "#     )\n",
        "    \n",
        "#     val_accuracy = model2.evaluate(val_gen)[1]\n",
        "#     print(f\"\\nFold {fold + 1} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "#     fold_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "# average_accuracy = sum(fold_accuracies) / k\n",
        "# print(f\"All folds validation accuracies: {fold_accuracies}\")\n",
        "# print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
        "\n",
        "\n",
        "# calculate_and_print_time(start_time, \"Model 2 K-fold cross-validation\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def display_confusion_matrix(model, val_gen, plt_title):\n",
        "#     y_true = val_gen.classes\n",
        "#     print(f\"y_true : {y_true}\")\n",
        "#     y_pred = model.predict(val_gen)\n",
        "#     # print(f\"y_pred : {y_pred}\")\n",
        "#     y_pred_classes = y_pred.argmax(axis=1)\n",
        "    \n",
        "#     cm = confusion_matrix(y_true, y_pred_classes)\n",
        "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_gen.class_indices.keys())\n",
        "#     disp.plot(cmap='viridis')\n",
        "#     plt.title(plt_title)\n",
        "#     plt.show()\n",
        "\n",
        "# display_confusion_matrix(model2, val_gen, \"Confusion Matrix for model 2\")\n",
        "# calculate_and_print_time(global_start_time, \"All Notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### notes : model2 problem is not in the model architecture (varified this by trying on bachelor model architecture, got similar results) so the problem is most likely in the data, or some where else in the code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
