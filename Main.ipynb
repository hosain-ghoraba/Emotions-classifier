{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "\n",
        "dimensions of the layers  \n",
        "<br>\n",
        "layers in/out data\n",
        "<br>\n",
        "\n",
        "Present the selected classes with some samples of the inputs and the obtained results in the same file\n",
        "<br>\n",
        "\n",
        "modify dataset perc to use\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Milestone 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Packages installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in ./.venv/lib/python3.12/site-packages (0.3.11)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->kagglehub) (2025.1.31)\n",
            "Requirement already satisfied: imagehash in ./.venv/lib/python3.12/site-packages (4.3.2)\n",
            "Requirement already satisfied: PyWavelets in ./.venv/lib/python3.12/site-packages (from imagehash) (1.8.0)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from imagehash) (2.1.3)\n",
            "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (from imagehash) (11.2.1)\n",
            "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from imagehash) (1.15.2)\n",
            "Requirement already satisfied: tensorflow in ./.venv/lib/python3.12/site-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in ./.venv/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.venv/lib/python3.12/site-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from tensorflow) (78.1.1)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.9.2)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.12/site-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./.venv/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in ./.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.67.1)\n",
            "done installing packages\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub\n",
        "!pip install imagehash\n",
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "print(\"done installing packages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hosain/Desktop/DL project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-04-26 21:52:12.604984: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-26 21:52:12.608342: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-26 21:52:12.617884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745693532.633512  227482 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745693532.638209  227482 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1745693532.650582  227482 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745693532.650599  227482 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745693532.650601  227482 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745693532.650602  227482 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-26 21:52:12.654640: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done importing packages\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import kagglehub\n",
        "import random\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "print(\"done importing packages\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Data preparaion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done setting up dataset path\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    dataset_folder = kagglehub.dataset_download('hussainghoraba/emotions-dataset')\n",
        "    DATASET_PATH = os.path.join(dataset_folder, 'Dataset')\n",
        "elif 'KAGGLE_URL_BASE' in os.environ:\n",
        "    DATASET_PATH = '/kaggle/input/emotions-dataset/Dataset'\n",
        "elif 'VSCODE_PID' in os.environ:\n",
        "    DATASET_PATH = './Dataset'\n",
        "else:\n",
        "    raise Exception('Unknown environment')\n",
        "\n",
        "print(\"done setting up dataset path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the array type used to store the images, either NumPy or CuPy, depending on the GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_227482/1312203158.py:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1745693534.269206  227482 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
            "W0000 00:00:1745693534.269737  227482 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "# use CuPy if running on colab gpu or kaggle gpu\n",
        "ARRAY_TYPE = np\n",
        "PCA_TYPE = PCA\n",
        "if tf.test.is_gpu_available() and ('COLAB_GPU' in os.environ or 'KAGGLE_URL_BASE' in os.environ):\n",
        "    import cupy as cp\n",
        "    ARRAY_TYPE = cp\n",
        "\n",
        "    from cuml.decomposition import PCA as cuPCA\n",
        "    PCA_TYPE = cuPCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set random seed & some global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done setting up random seed\n"
          ]
        }
      ],
      "source": [
        "RANDOM_SEED = 42\n",
        "TAREGT_SIZE_TUPLE = (512, 512)\n",
        "random.seed(RANDOM_SEED)\n",
        "ARRAY_TYPE.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
        "print(\"done setting up random seed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset into memory without dups, and with correct size, and equalize the number of images in each class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images in dataset: 2125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading images into memory...:  98%|█████████▊| 116/118 [00:01<00:00, 79.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done loading images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "total_images_count = sum(len(files) for _, _, files in os.walk(DATASET_PATH))\n",
        "print(f\"Total images in dataset: {total_images_count}\")\n",
        "data = []\n",
        "dups_pairs = set()\n",
        "\n",
        "# load only a small percentage of the dataset, for faster testing while developing\n",
        "DATASET_PERC_TO_USE = 0.1 \n",
        "\n",
        "num_images_in_smallest_category = min(len(os.listdir(os.path.join(DATASET_PATH, folder))) for folder in os.listdir(DATASET_PATH))\n",
        "num_of_images_to_use_in_each_category = int(num_images_in_smallest_category * DATASET_PERC_TO_USE)\n",
        "num_of_categories = len(os.listdir(DATASET_PATH))\n",
        "total_images_to_load = int(num_images_in_smallest_category * num_of_categories * DATASET_PERC_TO_USE)\n",
        "\n",
        "with tqdm(total=total_images_to_load, desc=\"Loading images into memory...\") as pbar:\n",
        "    for subfolder in os.listdir(DATASET_PATH):\n",
        "        subfolder_path = os.path.join(DATASET_PATH, subfolder)\n",
        "        subfolder_hashes = {}\n",
        "\n",
        "        all_category_images = os.listdir(subfolder_path)\n",
        "        # we must use the same number of images from each category to avoid bias\n",
        "        images_to_load = random.sample(all_category_images, num_of_images_to_use_in_each_category)\n",
        "        \n",
        "        for img_file in images_to_load:\n",
        "            img_path = os.path.join(subfolder_path, img_file)\n",
        "            with Image.open(img_path) as img:\n",
        "                img = img.convert(\"RGB\").resize(TAREGT_SIZE_TUPLE)\n",
        "                img_arr = ARRAY_TYPE.array(img)\n",
        "                img_hash = imagehash.phash(img)\n",
        "            if img_hash not in subfolder_hashes.keys():\n",
        "                data.append({\"img_path\": img_path, \"label\": subfolder, \"img_arr\": img_arr})\n",
        "                # key : hash, value : img_path\n",
        "                subfolder_hashes[img_hash] = img_path\n",
        "            else:\n",
        "                existing_duplicate = subfolder_hashes[img_hash]\n",
        "                dups_pairs.add((img_path, existing_duplicate))\n",
        "            pbar.update(1)\n",
        "        \n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# display dups\n",
        "for dup_pair in dups_pairs:\n",
        "    print(f\"Duplicate images found: {dup_pair[0]} and {dup_pair[1]}\")\n",
        "    img1 = Image.open(dup_pair[0])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img1)\n",
        "    plt.title(os.path.basename(dup_pair[0]))\n",
        "    plt.axis('off')\n",
        "    img2 = Image.open(dup_pair[1])\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img2)\n",
        "    plt.title(os.path.basename(dup_pair[1]))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "print(\"done loading images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Test/Val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C_b_7bkU03Y",
        "outputId": "7f2ab7fd-d76a-42c0-ac0d-fee917e716d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 81\n",
            "Validation set size: 23\n",
            "Test set size: 12\n",
            "\n",
            "training set:\n",
            "label\n",
            "Happy      21\n",
            "Sad        20\n",
            "Neutral    20\n",
            "Angry      20\n",
            "Name: count, dtype: int64\n",
            "\n",
            "validation set:\n",
            "label\n",
            "Sad        6\n",
            "Neutral    6\n",
            "Angry      6\n",
            "Happy      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "test set:\n",
            "label\n",
            "Neutral    3\n",
            "Happy      3\n",
            "Sad        3\n",
            "Angry      3\n",
            "Name: count, dtype: int64\n",
            "done splitting dataset into train, val, test\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=(1 - train_ratio), stratify=df['label'], random_state=RANDOM_SEED)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=(test_ratio / (test_ratio + val_ratio)), stratify=temp_df['label'], random_state=RANDOM_SEED)\n",
        "\n",
        "# Print the sizes of each split\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "print(\"\\ntraining set:\")\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "print(\"\\nvalidation set:\")\n",
        "print(val_df['label'].value_counts())\n",
        "\n",
        "print(\"\\ntest set:\")\n",
        "print(test_df['label'].value_counts())\n",
        "\n",
        "\n",
        "print(\"done splitting dataset into train, val, test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgDAdbjXbtMy"
      },
      "source": [
        "## **Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classes & Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementation is optimized for both cpu & gpu by using matrix multiplication\n",
        "def Relu(x):\n",
        "    return ARRAY_TYPE.maximum(0, x)\n",
        "\n",
        "def extend_depth(filters, depth):\n",
        "    return ARRAY_TYPE.stack([filters] * depth, axis=-1)\n",
        "\n",
        "# form the image windows to be multiplied with the filters, then flatten them for fast matrix multiplication\n",
        "def get_image_flattened_windows(input_data, filter_size):\n",
        "    input_h, input_w, input_c = input_data.shape\n",
        "    output_h = input_h - filter_size + 1\n",
        "    output_w = input_w - filter_size + 1\n",
        "\n",
        "    col = ARRAY_TYPE.zeros((output_h * output_w, filter_size * filter_size * input_c))\n",
        "    i = 0\n",
        "    for h in range(output_h):\n",
        "        for w in range(output_w):\n",
        "            cube = input_data[h:h+filter_size, w:w+filter_size, :]\n",
        "            col[i, :] = cube.flatten()\n",
        "            i += 1\n",
        "    return col\n",
        "\n",
        "class ConvLayer:\n",
        "    def __init__(self, filter_size=3, num_filters=5, filter_weights=None):\n",
        "        self.filter_size = filter_size\n",
        "        self.num_filters = num_filters\n",
        "        if filter_weights is None:\n",
        "            filter_weights = ARRAY_TYPE.random.normal(size=(self.num_filters, self.filter_size, self.filter_size)) * 0.1\n",
        "        self.filter_weights = filter_weights\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        input_h, input_w, input_c = input_image.shape\n",
        "\n",
        "        output_h = input_h - self.filter_size + 1\n",
        "        output_w = input_w - self.filter_size + 1\n",
        "\n",
        "        # extend the filter weights depth to match the input channels\n",
        "        deep_filters = extend_depth(self.filter_weights, input_c)\n",
        "\n",
        "        filters_reshaped = deep_filters.reshape(self.num_filters, -1)\n",
        "        input_col = get_image_flattened_windows(input_image, self.filter_size)\n",
        "        output_flat = filters_reshaped @ input_col.T\n",
        "        output = output_flat.T.reshape(output_h, output_w, self.num_filters)\n",
        "        return output\n",
        "    \n",
        "\n",
        "class PoolingLayer:\n",
        "    def __init__(self, pool_size=2, pool_type='MAX'):\n",
        "        if pool_type not in ['MAX', 'AVERAGE']:\n",
        "            raise ValueError(\"pool_type must be either 'MAX' or 'AVERAGE'\")\n",
        "        self.pool_size = pool_size\n",
        "        self.pool_type = pool_type\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        input_h, input_w, input_c = input_image.shape\n",
        "        pool_size = self.pool_size\n",
        "\n",
        "        output_h = input_h // pool_size\n",
        "        output_w = input_w // pool_size\n",
        "\n",
        "        output = ARRAY_TYPE.zeros((output_h, output_w, input_c))\n",
        "\n",
        "        for i in range(output_h):\n",
        "            for j in range(output_w):\n",
        "                h_start = i * pool_size\n",
        "                h_end = h_start + pool_size\n",
        "                w_start = j * pool_size\n",
        "                w_end = w_start + pool_size\n",
        "\n",
        "                pool_window = input_image[h_start:h_end, w_start:w_end, :]\n",
        "\n",
        "                if self.pool_type == 'MAX':\n",
        "                    output[i, j, :] = ARRAY_TYPE.max(pool_window, axis=(0, 1))\n",
        "                elif self.pool_type == 'AVERAGE':\n",
        "                    output[i, j, :] = ARRAY_TYPE.mean(pool_window, axis=(0, 1))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_image_features(image, filter_weights):\n",
        "    conv1_outpup = ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights).forward(image)\n",
        "    pool1_output = PoolingLayer(pool_size=2, pool_type='MAX').forward(conv1_outpup)\n",
        "    relu1_output = Relu(pool1_output)\n",
        "\n",
        "    conv2_outpup = ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights).forward(relu1_output)\n",
        "    pool2_output = PoolingLayer(pool_size=2, pool_type='MAX').forward(conv2_outpup)\n",
        "    relu2_output = Relu(pool2_output)\n",
        "\n",
        "    conv3_outpup = ConvLayer(filter_size=3, num_filters=5, filter_weights=filter_weights).forward(relu2_output)\n",
        "    pool3_output = PoolingLayer(pool_size=2, pool_type='MAX').forward(conv3_outpup)\n",
        "    relu3_output = Relu(pool3_output)\n",
        "\n",
        "    flattened = relu3_output.flatten()\n",
        "    return flattened\n",
        "\n",
        "\n",
        "def custom_kmeans(data, k):\n",
        "    # data is a 2d array, each row is an array containing \n",
        "    # the extracted features of images passed to model 1\n",
        "\n",
        "    rows_count = data.shape[0]\n",
        "\n",
        "    # Initialize centroids randomly\n",
        "    centroids = data[ARRAY_TYPE.random.choice(rows_count, k, replace=False)]\n",
        "\n",
        "    while True:\n",
        "        # assign each sample to the nearest centroid\n",
        "        distances = ARRAY_TYPE.linalg.norm(data[:, ARRAY_TYPE.newaxis] - centroids, axis=2)\n",
        "        labels = ARRAY_TYPE.argmin(distances, axis=1) \n",
        "\n",
        "        # update centroids \n",
        "        new_centroids = ARRAY_TYPE.array([data[labels == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "        # breack if difference between new and old centroids is small\n",
        "        tolerance = 1e-4\n",
        "        if ARRAY_TYPE.all(ARRAY_TYPE.linalg.norm(new_centroids - centroids, axis=1) < tolerance):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return centroids, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the model to extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features...: 100%|██████████| 116/116 [01:57<00:00,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "appling PCA...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "n_components=128 must be between 0 and min(n_samples, n_features)=116 with svd_solver='full'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# apply PCA to reduce the dimensionality of each feature array to 128 dimensions\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mappling PCA...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m features_arrays = \u001b[43mPCA_TYPE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_arrays\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL project/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL project/.venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL project/.venv/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:468\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    447\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    448\u001b[39m \n\u001b[32m    449\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    466\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL project/.venv/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:542\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcovariance_eigh\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_truncated(X, n_components, xp)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DL project/.venv/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:556\u001b[39m, in \u001b[36mPCA._fit_full\u001b[39m\u001b[34m(self, X, n_components, xp, is_array_api_compliant)\u001b[39m\n\u001b[32m    552\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    553\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmle\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is only supported if n_samples >= n_features\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    554\u001b[39m         )\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m <= n_components <= \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    557\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be between 0 and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    559\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    560\u001b[39m     )\n\u001b[32m    562\u001b[39m \u001b[38;5;28mself\u001b[39m.mean_ = xp.mean(X, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: n_components=128 must be between 0 and min(n_samples, n_features)=116 with svd_solver='full'"
          ]
        }
      ],
      "source": [
        "filter_weights = ARRAY_TYPE.stack([\n",
        "    ARRAY_TYPE.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]),\n",
        "    ARRAY_TYPE.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),\n",
        "    ARRAY_TYPE.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n",
        "    ARRAY_TYPE.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),\n",
        "    ARRAY_TYPE.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "], axis=0)\n",
        "\n",
        "# extract features from a single image to know their number,\n",
        "# because it is needed to create the array that holds all features\n",
        "single_image = df.iloc[0]['img_arr']\n",
        "num_features = extract_image_features(single_image, filter_weights).shape[0]\n",
        "\n",
        "features_arrays = ARRAY_TYPE.zeros((len(df), num_features))\n",
        "with tqdm(total=len(df), desc=\"Extracting features...\") as pbar:\n",
        "    for i in range(len(df)):\n",
        "        img_arr = df.iloc[i]['img_arr']\n",
        "        features_arrays[i, :] = extract_image_features(img_arr, filter_weights)\n",
        "        pbar.update(1)\n",
        "\n",
        "# downsample the features arrays to only 128 dimensions\n",
        "# use PCA only if we have more than 128 feature arrays ,\n",
        "# this is because in PCA, the number of components (which is 128 in our case) \n",
        "# must be less than the number of samples.\n",
        "# if we have less than 128 feature arrays, we will just select random 128 features\n",
        "if features_arrays.shape[0] >= 128:\n",
        "    print(f\"appling PCA...\")\n",
        "    features_arrays = PCA_TYPE(n_components=128).fit_transform(features_arrays)\n",
        "else:\n",
        "    print(f\"selecting random 128 features from {features_arrays.shape[0]} features\")\n",
        "    selected_indices = ARRAY_TYPE.random.choice(features_arrays.shape[1], 128, replace=False)\n",
        "    features_arrays = features_arrays[:, selected_indices]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'features_arrays' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m num_classes = \u001b[38;5;28mlen\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].unique())\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m centroids, labels = custom_kmeans(\u001b[43mfeatures_arrays\u001b[49m, k= num_classes)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcentroids shape:\u001b[39m\u001b[33m\"\u001b[39m, centroids.shape)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlabels shape:\u001b[39m\u001b[33m\"\u001b[39m, labels.shape)\n",
            "\u001b[31mNameError\u001b[39m: name 'features_arrays' is not defined"
          ]
        }
      ],
      "source": [
        "num_classes = len(df['label'].unique())\n",
        "centroids, labels = custom_kmeans(features_arrays, k= num_classes)\n",
        "\n",
        "print(\"centroids shape:\", centroids.shape)\n",
        "print(\"labels shape:\", labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Function to load and preprocess an image\n",
        "# def load_image(url=None, path=None, target_size=(512, 512)):\n",
        "#     \"\"\"Load an image from URL or local path and preprocess it.\"\"\"\n",
        "#     if url:\n",
        "#         response = requests.get(url)\n",
        "#         img = Image.open(BytesIO(response.content))\n",
        "#     elif path:\n",
        "#         img = Image.open(path)\n",
        "#     else:\n",
        "#         # Create a sample image if no source is provided\n",
        "#         img = Image.new('RGB', target_size, color=(73, 109, 137))\n",
        "\n",
        "#     # Resize the image\n",
        "#     img = img.resize(target_size)\n",
        "\n",
        "#     # Convert to numpy array\n",
        "#     img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "#     return img_array\n",
        "\n",
        "# # Test with a sample image (you can replace this URL with your own image)\n",
        "# # Using a sample image URL - replace with your own image or add code to load a local file\n",
        "# sample_image_url = \"https://images.wallpapersden.com/image/download/cat-green-eyed-muzzle_aWhnZ5SZmpqtpaSklGpmZ61qZmc.jpg\"\n",
        "# try:\n",
        "#     img = load_image(url=sample_image_url)\n",
        "# except:\n",
        "#     # If the URL doesn't work, create a simple test image\n",
        "#     img = np.zeros((512, 512, 3))\n",
        "#     # Add some shapes for testing\n",
        "#     img[100:400, 100:400, 0] = 1.0  # Red square\n",
        "#     img[200:300, 200:300, 1] = 1.0  # Green square inside red\n",
        "\n",
        "# Initialize a ConvLayer with the predefined filters\n",
        "conv_layer = ConvLayer(filter_size=3, num_filters=5, filter_weights=filters_3d)\n",
        "\n",
        "# # Apply convolution\n",
        "# feature_maps = conv_layer.forward(img)\n",
        "\n",
        "# # Visualize the original image and the feature maps\n",
        "# plt.figure(figsize=(15, 8))\n",
        "\n",
        "# # Original image\n",
        "# plt.subplot(2, 3, 1)\n",
        "# plt.imshow(img)\n",
        "# plt.title('Original Image')\n",
        "# plt.axis('off')\n",
        "\n",
        "# # Feature maps\n",
        "# filter_names = ['Box Filter (a)', 'Identity Filter (b)', 'Sobel X (c)', 'Sobel Y (d)', 'Sharpening (e)']\n",
        "\n",
        "# for i in range(5):\n",
        "#     plt.subplot(2, 3, i+2)\n",
        "#     # Normalize the feature map for better visualization\n",
        "#     feature_map = feature_maps[:, :, i]\n",
        "#     feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-8)\n",
        "#     plt.imshow(feature_map, cmap='viridis')\n",
        "#     plt.title(f'Filter {i+1}: {filter_names[i]}')\n",
        "#     plt.axis('off')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # Test individual filters\n",
        "# print(\"\\nTesting individual filters:\")\n",
        "# for i, (filter_name, filter_3d) in enumerate(zip(filter_names, [filter_a_3d, filter_b_3d, filter_c_3d, filter_d_3d, filter_e_3d])):\n",
        "#     # Create a ConvLayer with a single filter\n",
        "#     single_filter = np.expand_dims(filter_3d, axis=0)\n",
        "#     conv_layer_single = ConvLayer(filter_size=3, num_filters=1, filter_weights=single_filter)\n",
        "\n",
        "#     # Apply convolution\n",
        "#     feature_map = conv_layer_single.forward(img)\n",
        "\n",
        "#     print(f\"Filter {i+1} ({filter_name}) applied successfully.\")\n",
        "\n",
        "print(\"done model 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7VknR2cbydf"
      },
      "source": [
        "## **Model 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "xFDVzn_qb359",
        "outputId": "a0c234e1-4fc7-4dfb-f08e-32af3e76f40a"
      },
      "outputs": [],
      "source": [
        "# TA Dinah : \"You should have 5 convolution blocks each with 3 layers.\n",
        "#  3 convolution layers are simply 3 different filters on the same stage.\n",
        "#  A convolution block is some conv filters (layers) followed by an activation function and then a max pooling. \n",
        "# All convolution filters in the same block need to have the same size.\"\n",
        "\n",
        "def create_model2():\n",
        "    model = models.Sequential([\n",
        "    \n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid', input_shape=(TAREGT_SIZE_TUPLE[0], TAREGT_SIZE_TUPLE[1], 3)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(32, (5, 5), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.Conv2D(16, (7, 7), activation='relu', padding='valid'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    # 128 is OUR CHOICE for the number of neurons in the hidden layer (not specified in the project description)\n",
        "    layers.Dense(128, activation='sigmoid'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model2 = create_model2()\n",
        "\n",
        "\n",
        "print(\"done building model 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Early stopping to prevent overfitting (for the BONUS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                            patience=2,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr = 0.00001,\n",
        "                                            verbose = 1)\n",
        "\n",
        "early_stoping = EarlyStopping(monitor='val_loss',patience= 3,restore_best_weights=True,verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data augmentation (for the BONUS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# thiss will rotate the images by up to 20 degrees, also will increase the \n",
        "# dataset size on the fly while training \n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4GKqc0Yfl5e",
        "outputId": "7862dc2e-3b93-4508-d04a-01be54e7f565"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "\n",
        "# print trining dataset size before and after augmentation\n",
        "print(f\"Training dataset size before augmentation: {len(train_df)}\")\n",
        "train_gen = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    target_size=TAREGT_SIZE_TUPLE,\n",
        "    x_col='img_path',  \n",
        "    y_col='label',    \n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', \n",
        "    shuffle=True\n",
        ")\n",
        "print(f\"Training dataset size after augmentation: {len(train_gen)}\")\n",
        "\n",
        "val_gen = val_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    target_size=TAREGT_SIZE_TUPLE,\n",
        "    x_col='img_path',\n",
        "    y_col='label',\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "print(f\"images sizes passed to image data generator: {train_gen.image_shape}\")\n",
        "\n",
        "test_gen = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    target_size=TAREGT_SIZE_TUPLE,\n",
        "    x_col='img_path',\n",
        "    y_col='label',\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "model2_history = model2.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[early_stoping, learning_rate_reduction],\n",
        "    epochs=epochs,\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model2.evaluate(test_gen)\n",
        "print(f\"Val accuracy: {model2_history.history['val_accuracy'][-1]:.2f}\")\n",
        "print(f\"Test accuracy: {test_acc:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"done training model 2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "JMOPXDv7hYqe",
        "outputId": "f4a6c5ed-9f4e-4057-9374-fa663d72add1"
      },
      "outputs": [],
      "source": [
        "# # Get class indices and reverse mapping\n",
        "# class_indices = test_gen.class_indices\n",
        "# class_labels = list(class_indices.keys())\n",
        "\n",
        "# # Get predictions for the test set\n",
        "# preds = model.predict(test_gen)\n",
        "# predicted_classes = np.argmax(preds, axis=1)\n",
        "# true_classes = test_gen.classes\n",
        "# filenames = test_gen.filenames\n",
        "\n",
        "# # Display 10 random images with predictions\n",
        "# num_images = 10\n",
        "# indices = np.random.choice(len(filenames), num_images, replace=False)\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "\n",
        "# for i, idx in enumerate(indices):\n",
        "#     img_path = os.path.join(test_dir, filenames[idx])\n",
        "#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
        "#     img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "\n",
        "#     plt.subplot(2, 5, i + 1)\n",
        "#     plt.imshow(img_array)\n",
        "#     plt.axis('off')\n",
        "#     true_label = class_labels[true_classes[idx]]\n",
        "#     predicted_label = class_labels[predicted_classes[idx]]\n",
        "#     title_color = \"green\" if true_label == predicted_label else \"red\"\n",
        "#     plt.title(f\"True: {true_label}\\nPred: {predicted_label}\", color=title_color)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "print(\"done displaying predictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Milestone 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 1**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy vs iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to be implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **model 2**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy vs iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(model2_history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(model2_history.history['val_accuracy'], label='val accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "k = 4\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "fold_accuracies = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "    print(f\"\\nStarting fold {fold + 1}/{k}...\")\n",
        "    \n",
        "    train_data = df.iloc[train_idx]\n",
        "    val_data = df.iloc[val_idx]\n",
        "    \n",
        "    train_gen = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_data,\n",
        "        x_col='img_path',\n",
        "        y_col='label',\n",
        "        target_size=TAREGT_SIZE_TUPLE,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "    \n",
        "    val_gen = val_datagen.flow_from_dataframe(\n",
        "        dataframe=val_data,\n",
        "        x_col='img_path',\n",
        "        y_col='label',\n",
        "        target_size=TAREGT_SIZE_TUPLE,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    model2 = create_model2()\n",
        "    \n",
        "    model2_history = model2.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[early_stoping, learning_rate_reduction],\n",
        "        epochs=epochs,\n",
        "    )\n",
        "    \n",
        "    val_accuracy = model2.evaluate(val_gen)[1]\n",
        "    print(f\"\\nFold {fold + 1} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    fold_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "average_accuracy = sum(fold_accuracies) / k\n",
        "print(f\"All folds validation accuracies: {fold_accuracies}\")\n",
        "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_confusion_matrix(model, val_gen, plt_title):\n",
        "    y_true = val_gen.classes\n",
        "    print(f\"y_true : {y_true}\")\n",
        "    y_pred = model.predict(val_gen)\n",
        "    # print(f\"y_pred : {y_pred}\")\n",
        "    y_pred_classes = y_pred.argmax(axis=1)\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred_classes)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_gen.class_indices.keys())\n",
        "    disp.plot(cmap='viridis')\n",
        "    plt.title(plt_title)\n",
        "    plt.show()\n",
        "\n",
        "display_confusion_matrix(model2, val_gen, \"Confusion Matrix for model 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### notes : model2 problem is not in the model architecture (varified this by trying on bachelor model architecture, got similar results) so the problem is most likely in the data, or some where else in the code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
